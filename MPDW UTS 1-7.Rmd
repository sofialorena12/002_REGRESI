---
title: "MPDW UTS 1-7"
author: "Alista Sava Davina"
date: "2024-10-09"
output: html_document
---

# Pertemuan 1

```{r}
library("forecast")
library("graphics")
library("TTR")
library("TSA")
```

## Impor Data

```{r}
#install.packages("rio") #install jika belum ada
library(rio)
data1 <- import("https://raw.githubusercontent.com/rizkynurhambali/Praktikum-MPDW-2324/main/Pertemuan%201/Data_1.csv")
data2 <- import("https://raw.githubusercontent.com/rizkynurhambali/Praktikum-MPDW-2324/main/Pertemuan%201/Data_2.csv")
```

## Eksplorasi Data

Melihat data menggunakan fungsi `View()`, struktur data menggunakan fungsi `str()`, dan dimensi data menggunakan fungsi `dim()`.

```{r}
View(data1)
str(data1)
dim(data1)
```

Mengubah data agar terbaca sebagai data deret waktu dengan fungsi `ts()` .

```{r}
data1.ts <- ts(data1$Yt)
data2.ts <- ts(data2$Sales)
```

Menampilkan ringkasan data

```{r}
summary(data1.ts)
summary(data2.ts)
```

Membuat plot data deret waktu

```{r}
ts.plot(data1.ts, xlab="Time Period ", ylab="Reading", 
        main = "Time Series Plot")
points(data1.ts)
```

```{r}
ts.plot(data2.ts, xlab="Time Period ", ylab="Sales", 
        main = "Time Series Plot of Sales")
points(data2.ts)
```

Menyimpan plot

```{r}
#menyimpan plot
#dev.copy(png, "eksplorasi.png")
#dev.off()
```

## Single Moving Average & Double Moving Average

### Pembagian Data

Pembagian data latih dan data uji dilakukan dengan perbandingan 80% data latih dan 20% data uji.

```{r}
#membagi data latih dan data uji
training_ma <- data2[1:96,]
testing_ma <- data2[97:120,]
train_ma.ts <- ts(training_ma$Sales)
test_ma.ts <- ts(testing_ma$Sales)
```

### Eksplorasi Data

Eksplorasi data dilakukan pada keseluruhan data, data latih serta data uji menggunakan plot data deret waktu.

```{r}
#eksplorasi keseluruhan data
plot(data2.ts, col="red",main="Plot semua data")
points(data2.ts)

#eksplorasi data latih
plot(train_ma.ts, col="blue",main="Plot data latih")
points(train_ma.ts)

#eksplorasi data uji
plot(test_ma.ts, col="blue",main="Plot data uji")
points(test_ma.ts)
```

Eksplorasi data juga dapat dilakukan menggunakan package `ggplot2` dengan terlebih dahulu memanggil library *package* `ggplot2`.

```{r}
#Eksplorasi dengan GGPLOT
library(ggplot2)
ggplot() + 
  geom_line(data = training_ma, aes(x = Period, y = Sales, col = "Data Latih")) +
  geom_line(data = testing_ma, aes(x = Period, y = Sales, col = "Data Uji")) +
  labs(x = "Periode Waktu", y = "Sales", color = "Legend") +
  scale_colour_manual(name="Keterangan:", breaks = c("Data Latih", "Data Uji"),
                      values = c("blue", "red")) + 
  theme_bw() + theme(legend.position = "bottom",
                     plot.caption = element_text(hjust=0.5, size=12))
```

### Single Moving Average (SMA)

Ide dasar dari Single Moving Average (SMA) adalah data suatu periode dipengaruhi oleh data periode sebelumnya. Metode pemulusan ini cocok digunakan untuk pola data stasioner atau konstan. Prinsip dasar metode pemulusan ini adalah data pemulusan pada periode ke-t merupakan rata rata dari m buah data pada periode ke-t hingga periode ke (t-m+1). Data pemulusan pada periode ke-t selanjutnya digunakan sebagai nilai peramalan pada periode ke t+1

Pemulusan menggunakan metode SMA dilakukan dengan fungsi `SMA()`. Dalam hal ini akan dilakukan pemulusan dengan parameter `m=4`.

```{r}
data.sma<-SMA(train_ma.ts, n=4)
data.sma
```

Data pemulusan pada periode ke-t selanjutnya digunakan sebagai nilai peramalan pada periode ke t+1 sehingga hasil peramalan 1 periode kedepan adalah sebagai berikut.

```{r}
data.ramal<-c(NA,data.sma)
data.ramal #forecast 1 periode ke depan
```

Selanjutnya akan dilakukan peramalan sejumlah data uji yaitu 24 periode. Pada metode SMA, hasil peramalan 24 periode ke depan akan bernilai sama dengan hasil peramalan 1 periode kedepan. Dalam hal ini akan dilakukan pengguabungan data aktual train, data hasil pemulusan dan data hasil ramalan 24 periode kedepan.

```{r}
data.gab<-cbind(aktual=c(train_ma.ts,rep(NA,24)),pemulusan=c(data.sma,rep(NA,24)),ramalan=c(data.ramal,rep(data.ramal[length(data.ramal)],23)))
data.gab #forecast 24 periode ke depan
```

Adapun plot data deret waktu dari hasil peramalan yang dilakukan adalah sebagai berikut.

```{r}
ts.plot(data2.ts, xlab="Time Period ", ylab="Sales", main= "SMA N=4 Data Sales")
points(data2.ts)
lines(data.gab[,2],col="green",lwd=2)
lines(data.gab[,3],col="red",lwd=2)
legend("topleft",c("data aktual","data pemulusan","data peramalan"), lty=8, col=c("black","green","red"), cex=0.5)
```

Selanjutnya perhitungan akurasi dilakukan dengan ukuran akurasi *Sum Squares Error* (SSE), *Mean Square Error* (MSE) dan *Mean Absolute Percentage Error* (MAPE). Perhitungan akurasi dilakukan baik pada data latih maupun pada data uji.

```{r}
#Menghitung nilai keakuratan data latih
error_train.sma = train_ma.ts-data.ramal[1:length(train_ma.ts)]
SSE_train.sma = sum(error_train.sma[5:length(train_ma.ts)]^2)
MSE_train.sma = mean(error_train.sma[5:length(train_ma.ts)]^2)
MAPE_train.sma = mean(abs((error_train.sma[5:length(train_ma.ts)]/train_ma.ts[5:length(train_ma.ts)])*100))

akurasi_train.sma <- matrix(c(SSE_train.sma, MSE_train.sma, MAPE_train.sma))
row.names(akurasi_train.sma)<- c("SSE", "MSE", "MAPE")
colnames(akurasi_train.sma) <- c("Akurasi m = 4")
akurasi_train.sma
```

Dalam hal ini nilai MAPE data latih pada metode pemulusan SMA kurang dari 2%, nilai ini dapat dikategorikan sebagai nilai akurasi yang sangat baik. Selanjutnya dilakukan perhitungan nilai MAPE data uji pada metde pemulusan SMA.

```{r}
#Menghitung nilai keakuratan data uji
error_test.sma = test_ma.ts-data.gab[97:120,3]
SSE_test.sma = sum(error_test.sma^2)
MSE_test.sma = mean(error_test.sma^2)
MAPE_test.sma = mean(abs((error_test.sma/test_ma.ts*100)))

akurasi_test.sma <- matrix(c(SSE_test.sma, MSE_test.sma, MAPE_test.sma))
row.names(akurasi_test.sma)<- c("SSE", "MSE", "MAPE")
colnames(akurasi_test.sma) <- c("Akurasi m = 4")
akurasi_test.sma
```

Perhitungan akurasi menggunakan data latih menghasilkan nilai MAPE yang kurang dari 10% sehingga nilai akurasi ini dapat dikategorikan sebagai sangat baik.

### Double Moving Average (DMA)

Metode pemulusan Double Moving Average (DMA) pada dasarnya mirip dengan SMA. Namun demikian, metode ini lebih cocok digunakan untuk pola data trend. Proses pemulusan dengan rata rata dalam metode ini dilakukan sebanyak 2 kali.

```{r}
dma <- SMA(data.sma, n = 4)
At <- 2*data.sma - dma
Bt <- 2/(4-1)*(data.sma - dma)
data.dma<- At+Bt
data.ramal2<- c(NA, data.dma)

t = 1:24
f = c()

for (i in t) {
  f[i] = At[length(At)] + Bt[length(Bt)]*(i)
}

data.gab2 <- cbind(aktual = c(train_ma.ts,rep(NA,24)), pemulusan1 = c(data.sma,rep(NA,24)),pemulusan2 = c(data.dma, rep(NA,24)),At = c(At, rep(NA,24)), Bt = c(Bt,rep(NA,24)),ramalan = c(data.ramal2, f[-1]))
data.gab2

```

Hasil pemulusan menggunakan metode DMA divisualisasikan sebagai berikut

```{r}
ts.plot(data2.ts, xlab="Time Period ", ylab="Sales", main= "DMA N=4 Data Sales")
points(data2.ts)
lines(data.gab2[,3],col="green",lwd=2)
lines(data.gab2[,6],col="red",lwd=2)
legend("topleft",c("data aktual","data pemulusan","data peramalan"), lty=8, col=c("black","green","red"), cex=0.8)

```

Selanjutnya perhitungan akurasi dilakukan baik pada data latih maupun data uji. Perhitungan akurasi dilakukan dengan ukuran akurasi SSE, MSE dan MAPE.

```{r}
#Menghitung nilai keakuratan data latih
error_train.dma = train_ma.ts-data.ramal2[1:length(train_ma.ts)]
SSE_train.dma = sum(error_train.dma[8:length(train_ma.ts)]^2)
MSE_train.dma = mean(error_train.dma[8:length(train_ma.ts)]^2)
MAPE_train.dma = mean(abs((error_train.dma[8:length(train_ma.ts)]/train_ma.ts[8:length(train_ma.ts)])*100))

akurasi_train.dma <- matrix(c(SSE_train.dma, MSE_train.dma, MAPE_train.dma))
row.names(akurasi_train.dma)<- c("SSE", "MSE", "MAPE")
colnames(akurasi_train.dma) <- c("Akurasi m = 4")
akurasi_train.dma
```

Perhitungan akurasi pada data latih menggunakan nilai MAPE menghasilkan nilai MAPE yang kurang dari 10% sehingga dikategorikan sangat baik. Selanjutnya, perhitungan nilai akurasi dilakukan pada data uji.

```{r}
#Menghitung nilai keakuratan data uji
error_test.dma = test_ma.ts-data.gab2[97:120,6]
SSE_test.dma = sum(error_test.dma^2)
MSE_test.dma = mean(error_test.dma^2)
MAPE_test.dma = mean(abs((error_test.dma/test_ma.ts*100)))

akurasi_test.dma <- matrix(c(SSE_test.dma, MSE_test.dma, MAPE_test.dma))
row.names(akurasi_test.dma)<- c("SSE", "MSE", "MAPE")
colnames(akurasi_test.dma) <- c("Akurasi m = 4")
akurasi_test.dma
```

Perhitungan akurasi menggunakan data latih menghasilkan nilai MAPE yang kurang dari 10% sehingga nilai akurasi ini dapat dikategorikan sebagai sangat baik.

Pada data latih, metode SMA lebih baik dibandingkan dengan metode DMA, sedangkan pada data uji, metode DMA lebih baik dibandingkan SMA

## Single Exponential Smoothing & Double Exponential Smoothing

Metode *Exponential Smoothing* adalah metode pemulusan dengan melakukan pembobotan menurun secara eksponensial. Nilai yang lebih baru diberi bobot yang lebih besar dari nilai terdahulu. Terdapat satu atau lebih parameter pemulusan yang ditentukan secara eksplisit, dan hasil pemilihan parameter tersebut akan menentukan bobot yang akan diberikan pada nilai pengamatan. Ada dua macam model, yaitu model tunggal dan ganda.

### Pembagian Data

Pembagian data latih dan data uji dilakukan dengan perbandingan 80% data latih dan 20% data uji.

```{r}
#membagi training dan testing
training<-data1[1:40,]
testing<-data1[41:50,]
train.ts <- ts(training$Yt)
test.ts <- ts(testing$Yt)
```

### Eksplorasi

Eksplorasi dilakukan dengan membuat plot data deret waktu untuk keseluruhan data, data latih, dan data uji.

```{r}
#eksplorasi data
plot(data1.ts, col="black",main="Plot semua data")
points(data1.ts)

plot(train.ts, col="red",main="Plot data latih")
points(train.ts)

plot(test.ts, col="blue",main="Plot data uji")
points(test.ts)
```

Eksplorasi data juga dapat dilakukan menggunakan package `ggplot2` .

```{r}
#Eksplorasi dengan GGPLOT
library(ggplot2)
ggplot() + 
  geom_line(data = training, aes(x = Periode, y = Yt, col = "Data Latih")) +
  geom_line(data = testing, aes(x = Periode, y = Yt, col = "Data Uji")) +
  labs(x = "Periode Waktu", y = "Membaca", color = "Legend") +
  scale_colour_manual(name="Keterangan:", breaks = c("Data Latih", "Data Uji"),
                      values = c("blue", "red")) + 
  theme_bw() + theme(legend.position = "bottom",
                     plot.caption = element_text(hjust=0.5, size=12))
```

### SES

Single Exponential Smoothing merupakan metode pemulusan yang tepat digunakan untuk data dengan pola stasioner atau konstan.

Nilai pemulusan pada periode ke-t didapat dari persamaan:

$$
\tilde{y}_T=\lambda y_t+(1-\lambda)\tilde{y}_{T-1}
$$

Nilai parameter $\lambda$ adalah nilai antara 0 dan 1.

Nilai pemulusan periode ke-t bertindak sebagai nilai ramalan pada periode ke-$(T+\tau)$.

$$
\tilde{y}_{T+\tau}(T)=\tilde{y}_T
$$

Pemulusan dengan metode SES dapat dilakukan dengan dua fungsi dari *packages* berbeda, yaitu (1) fungsi `ses()` dari *packages* `forecast` dan (2) fungsi `HoltWinters` dari *packages* `stats` .

```{r}
#Cara 1 (fungsi ses)
ses.1 <- ses(train.ts, h = 10, alpha = 0.2)
plot(ses.1)
ses.1

ses.2<- ses(train.ts, h = 10, alpha = 0.7)
plot(ses.2)
ses.2
```

Untuk mendapatkan gambar hasil pemulusan pada data latih dengan fungsi `ses()` , perlu digunakan fungsi `autoplot()` dan `autolayer()` dari *library packages* `ggplot2` .

```{r}
autoplot(ses.1) +
  autolayer(fitted(ses.1), series="Fitted") +
  ylab("Membaca") + xlab("Periode")
```

Pada fungsi `ses()` , terdapat beberapa argumen yang umum digunakan, yaitu nilia `y` , `gamma` , `beta` , `alpha` , dan `h` .

Nilai `y` adalah nilai data deret waktu, `gamma` adalah parameter pemulusan untuk komponen musiman, `beta` adalah parameter pemulusan untuk tren, dan `alpha` adalah parameter pemulusan untuk stasioner, serta `h` adalah banyaknya periode yang akan diramalkan.

Kasus di atas merupakan contoh inisialisasi nilai parameter $\lambda$ dengan nilai `alpha` 0,2 dan 0,7 dan banyak periode data yang akan diramalkan adalah sebanyak 10 periode. Selanjutnya akan digunakan fungsi `HoltWinters()` dengan nilai inisialisasi parameter dan panjang periode peramalan yang sama dengan fungsi `ses()` .

```{r}
#Cara 2 (fungsi Holtwinter)
ses1<- HoltWinters(train.ts, gamma = FALSE, beta = FALSE, alpha = 0.2)
plot(ses1)

#ramalan
ramalan1<- forecast(ses1, h=10)
ramalan1

ses2<- HoltWinters(train.ts, gamma = FALSE, beta = FALSE, alpha = 0.7)
plot(ses2)

#ramalan
ramalan2<- forecast(ses2, h=10)
ramalan2
```

Fungsi `HoltWinters` memiliki argumen yang sama dengan fungsi `ses()` . Argumen-argumen kedua fungsi dapat dilihat lebih lanjut dengan `?ses()` atau `?HoltWinters` .

Nilai parameter $\alpha$ dari kedua fungsi dapat dioptimalkan menyesuaikan dari *error*-nya paling minimumnya. Caranya adalah dengan membuat parameter $\alpha =$ `NULL` .

```{r}
#SES
ses.opt <- ses(train.ts, h = 10, alpha = NULL)
plot(ses.opt)
ses.opt

#Lamda Optimum Holt Winter
sesopt<- HoltWinters(train.ts, gamma = FALSE, beta = FALSE,alpha = NULL)
sesopt
plot(sesopt)

#ramalan
ramalanopt<- forecast(sesopt, h=10)
ramalanopt
```

Setelah dilakukan peramalan, akan dilakukan perhitungan keakuratan hasil peramalan. Perhitungan akurasi ini dilakukan baik pada data latih dan data uji.

#### Akurasi Data Latih

Perhitungan akurasi data dapat dilakukan dengan cara langsung maupun manual. Secara langsung, nilai akurasi dapat diambil dari objek yang tersimpan pada hasil SES, yaitu *sum of squared errors* (SSE). Nilai akurasi lain dapat dihitung pula dari nilai SSE tersebut.

```{r}
#Keakuratan Metode
#Pada data training
SSE1<-ses1$SSE
MSE1<-ses1$SSE/length(train.ts)
RMSE1<-sqrt(MSE1)

akurasi1 <- matrix(c(SSE1,MSE1,RMSE1))
row.names(akurasi1)<- c("SSE", "MSE", "RMSE")
colnames(akurasi1) <- c("Akurasi lamda=0.2")
akurasi1

SSE2<-ses2$SSE
MSE2<-ses2$SSE/length(train.ts)
RMSE2<-sqrt(MSE2)

akurasi2 <- matrix(c(SSE2,MSE2,RMSE2))
row.names(akurasi2)<- c("SSE", "MSE", "RMSE")
colnames(akurasi2) <- c("Akurasi lamda=0.7")
akurasi2

#Cara Manual
fitted1<-ramalan1$fitted
sisaan1<-ramalan1$residuals
head(sisaan1)

resid1<-training$Yt-ramalan1$fitted
head(resid1)
```

```{r}
#Cara Manual
SSE.1=sum(sisaan1[2:length(train.ts)]^2)
SSE.1

MSE.1 = SSE.1/length(train.ts)
MSE.1

MAPE.1 = sum(abs(sisaan1[2:length(train.ts)]/train.ts[2:length(train.ts)])*
               100)/length(train.ts)
MAPE.1

akurasi.1 <- matrix(c(SSE.1,MSE.1,MAPE.1))
row.names(akurasi.1)<- c("SSE", "MSE", "MAPE")
colnames(akurasi.1) <- c("Akurasi lamda=0.2")
akurasi.1

fitted2<-ramalan2$fitted
sisaan2<-ramalan2$residuals
head(sisaan2)

resid2<-training$Yt-ramalan2$fitted
head(resid2)

SSE.2=sum(sisaan2[2:length(train.ts)]^2)
SSE.2

MSE.2 = SSE.2/length(train.ts)
MSE.2

MAPE.2 = sum(abs(sisaan2[2:length(train.ts)]/train.ts[2:length(train.ts)])*
               100)/length(train.ts)
MAPE.2

akurasi.2 <- matrix(c(SSE.2,MSE.2,MAPE.2))
row.names(akurasi.2)<- c("SSE", "MSE", "MAPE")
colnames(akurasi.2) <- c("Akurasi lamda=0.7")
akurasi.2
```

Berdasarkan nilai SSE, MSE, RMSE, dan MAPE di antara kedua parameter, nilai parameter $\lambda=0,2$ menghasilkan akurasi yang lebih baik dibanding $\lambda=0,7$ . Hal ini dilihat dari nilai masing-masing ukuran akurasi yang lebih kecil. Berdasarkan nilai MAPE-nya, hasil ini dapat dikategorikan sebagai peramalan sangat baik.

#### Akurasi Data Uji

Akurasi data uji dapat dihitung dengan cara yang hampir sama dengan perhitungan akurasi data latih.

```{r}
selisih1<-ramalan1$mean-testing$Yt
SSEtesting1<-sum(selisih1^2)
MSEtesting1<-SSEtesting1/length(testing)

selisih2<-ramalan2$mean-testing$Yt
SSEtesting2<-sum(selisih2^2)
MSEtesting2<-SSEtesting2/length(testing)

selisihopt<-ramalanopt$mean-testing$Yt
SSEtestingopt<-sum(selisihopt^2)
MSEtestingopt<-SSEtestingopt/length(testing)

akurasitesting1 <- matrix(c(SSEtesting1,SSEtesting2,SSEtestingopt))
row.names(akurasitesting1)<- c("SSE1", "SSE2", "SSEopt")
akurasitesting1

akurasitesting2 <- matrix(c(MSEtesting1,MSEtesting2,MSEtestingopt))
row.names(akurasitesting2)<- c("MSE1", "MSE2", "MSEopt")
akurasitesting2
```

Selain dengan cara di atas, perhitungan nilai akurasi dapat menggunakan fungsi `accuracy()` dari *package* `forecast` . Penggunaannya yaitu dengan menuliskan `accuracy(hasil ramalan, kondisi aktual)` . Contohnya adalah sebagai berikut.

```{r}
#cara lain
accuracy(ramalanopt,testing$Yt)
```

### DES

Metode pemulusan *Double Exponential Smoothing* (DES) digunakan untuk data yang memiliki pola tren. Metode DES adalah metode semacam SES, hanya saja dilakukan dua kali, yaitu pertama untuk tahapan 'level' dan kedua untuk tahapan 'tren'. Pemulusan menggunakan metode ini akan menghasilkan peramalan tidak konstan untuk periode berikutnya.

Pemulusan dengan metode DES kali ini akan menggunakan fungsi `HoltWinters()` . Jika sebelumnya nilai argumen `beta` dibuat `FALSE` , kali ini argumen tersebut akan diinisialisasi bersamaan dengan nilai `alpha` .

```{r}
#Lamda=0.2 dan gamma=0.2
des.1<- HoltWinters(train.ts, gamma = FALSE, beta = 0.2, alpha = 0.2)
plot(des.1)

#ramalan
ramalandes1<- forecast(des.1, h=10)
ramalandes1

#Lamda=0.6 dan gamma=0.3
des.2<- HoltWinters(train.ts, gamma = FALSE, beta = 0.3, alpha = 0.6)
plot(des.2)

#ramalan
ramalandes2<- forecast(des.2, h=10)
ramalandes2
```

Selanjutnya jika ingin membandingkan plot data latih dan data uji adalah sebagai berikut.

```{r}
#Visually evaluate the prediction
plot(data1.ts)
lines(des.1$fitted[,1], lty=2, col="blue")
lines(ramalandes1$mean, col="red")
```

Untuk mendapatkan nilai parameter optimum dari DES, argumen `alpha` dan `beta` dapat dibuat `NULL` seperti berikut.

```{r}
#Lamda dan gamma optimum
des.opt<- HoltWinters(train.ts, gamma = FALSE)
des.opt
plot(des.opt)

#ramalan
ramalandesopt<- forecast(des.opt, h=10)
ramalandesopt
```

Selanjutnya akan dilakukan perhitungan akurasi pada data latih maupun data uji dengan ukuran akurasi SSE, MSE dan MAPE.

#### Akurasi Data Latih

```{r}
#Akurasi Data Training
ssedes.train1<-des.1$SSE
msedes.train1<-ssedes.train1/length(train.ts)
sisaandes1<-ramalandes1$residuals
head(sisaandes1)

mapedes.train1 <- sum(abs(sisaandes1[3:length(train.ts)]/train.ts[3:length(train.ts)])
                      *100)/length(train.ts)

akurasides.1 <- matrix(c(ssedes.train1,msedes.train1,mapedes.train1))
row.names(akurasides.1)<- c("SSE", "MSE", "MAPE")
colnames(akurasides.1) <- c("Akurasi lamda=0.2 dan gamma=0.2")
akurasides.1

ssedes.train2<-des.2$SSE
msedes.train2<-ssedes.train2/length(train.ts)
sisaandes2<-ramalandes2$residuals
head(sisaandes2)

mapedes.train2 <- sum(abs(sisaandes2[3:length(train.ts)]/train.ts[3:length(train.ts)])
                      *100)/length(train.ts)

akurasides.2 <- matrix(c(ssedes.train2,msedes.train2,mapedes.train2))
row.names(akurasides.2)<- c("SSE", "MSE", "MAPE")
colnames(akurasides.2) <- c("Akurasi lamda=0.6 dan gamma=0.3")
akurasides.2
```

Hasil akurasi dari data latih didapatkan skenario 2 dengan lamda=0.6 dan gamma=0.3 memiliki hasil yang lebih baik. Namun untuk kedua skenario dapat dikategorikan peramalan sangat baik berdasarkan nilai MAPE-nya.

#### Akurasi Data Uji

```{r}
#Akurasi Data Testing
selisihdes1<-ramalandes1$mean-testing$Yt
selisihdes1

SSEtestingdes1<-sum(selisihdes1^2)
MSEtestingdes1<-SSEtestingdes1/length(testing$Yt)
MAPEtestingdes1<-sum(abs(selisihdes1/testing$Yt)*100)/length(testing$Yt)

selisihdes2<-ramalandes2$mean-testing$Yt
selisihdes2

SSEtestingdes2<-sum(selisihdes2^2)
MSEtestingdes2<-SSEtestingdes2/length(testing$Yt)
MAPEtestingdes2<-sum(abs(selisihdes2/testing$Yt)*100)/length(testing$Yt)

selisihdesopt<-ramalandesopt$mean-testing$Yt
selisihdesopt

SSEtestingdesopt<-sum(selisihdesopt^2)
MSEtestingdesopt<-SSEtestingdesopt/length(testing$Yt)
MAPEtestingdesopt<-sum(abs(selisihdesopt/testing$Yt)*100)/length(testing$Yt)

akurasitestingdes <-
  matrix(c(SSEtestingdes1,MSEtestingdes1,MAPEtestingdes1,SSEtestingdes2,MSEtestingdes2,
           MAPEtestingdes2,SSEtestingdesopt,MSEtestingdesopt,MAPEtestingdesopt),
         nrow=3,ncol=3)
row.names(akurasitestingdes)<- c("SSE", "MSE", "MAPE")
colnames(akurasitestingdes) <- c("des ske1","des ske2","des opt")
akurasitestingdes
```

#### Perbandingan SES dan DES

```{r}
MSEfull <-
  matrix(c(MSEtesting1,MSEtesting2,MSEtestingopt,MSEtestingdes1,MSEtestingdes2,
           MSEtestingdesopt),nrow=3,ncol=2)
row.names(MSEfull)<- c("ske 1", "ske 2", "ske opt")
colnames(MSEfull) <- c("ses","des")
MSEfull
```

Kedua metode dapat dibandingkan dengan menggunakan ukuran akurasi yang sama. Contoh di atas adalah perbandingan kedua metode dengan ukuran akurasi MSE. Hasilnya didapatkan metode DES lebih baik dibandingkan metode SES dilihat dari MSE yang lebih kecil nilainya.

## Pemulusan Data Musiman

Pertama impor kembali data baru untuk latihan data musiman.

```{r}
#Import data
library(rio)
data3 <- import("https://raw.githubusercontent.com/rizkynurhambali/Praktikum-MPDW-2324/main/Pertemuan%201/Electric_Production.csv")
data3.ts <- ts(data3$Yt)
```

Selanjutnya melakukan pembagian data dan mengubahnya menjadi data deret waktu.

```{r}
#membagi data menjadi training dan testing
training<-data3[1:192,2]
testing<-data3[193:241,2]
training.ts<-ts(training, frequency = 13)
testing.ts<-ts(testing, frequency = 13)
```

Kemudian akan dilakukan eskplorasi dengan plot data deret waktu sebagai berikut.

```{r}
#Membuat plot time series
plot(data3.ts, col="red",main="Plot semua data")
points(data3.ts)

plot(training.ts, col="blue",main="Plot data latih")
points(training.ts)

plot(testing.ts, col="green",main="Plot data uji")
points(testing.ts)
```

Metode Holt-Winter untuk peramalan data musiman menggunakan tiga persamaan pemulusan yang terdiri atas persamaan untuk level $(L_t)$, trend $(B_t)$, dan komponen seasonal / musiman $(S_t)$ dengan parameter pemulusan berupa $\alpha$, $\beta$, dan $\gamma$. Metode Holt-Winter musiman terbagi menjadi dua, yaitu metode aditif dan metode multiplikatif. Perbedaan persamaan dan contoh datanya adalah sebagai berikut.

![](images/Perbandingan%20model%20aditif.png){width="609"}

![](images/example%20additive-multiplicative.png)

Pemulusan data musiman dengan metode Winter dilakukan menggunakan fungsi `HoltWinters()` dengan memasukkan argumen tambahan, yaitu `gamma()` dan `seasonal()` . Arguman `seasonal()` diinisialisasi menyesuaikan jenis musiman, aditif atau multiplikatif.

### Winter Aditif

Perhitungan dengan model aditif dilakukan jika plot data asli menunjukkan fluktuasi musiman yang relatif stabil (konstan).

#### Pemulusan

```{r}
#Pemulusan dengan winter aditif 
winter1 <- HoltWinters(training.ts,alpha=0.2,beta=0.1,gamma=0.1,seasonal = "additive")
winter1$fitted
xhat1 <- winter1$fitted[,2]

winter1.opt<- HoltWinters(training.ts, alpha= NULL,  beta = NULL, gamma = NULL, seasonal = "additive")
winter1.opt
winter1.opt$fitted
xhat1.opt <- winter1.opt$fitted[,2]
```

#### Peramalan

```{r}
#Forecast
forecast1 <- predict(winter1, n.ahead = 49)
forecast1.opt <- predict(winter1.opt, n.ahead = 49)
```

#### Plot Deret Waktu

```{r}
#Plot time series
plot(training.ts,main="Winter 0.2;0.1;0.1",type="l",col="black",
     xlim=c(1,25),pch=12)
lines(xhat1,type="l",col="red")
lines(xhat1.opt,type="l",col="blue")
lines(forecast1,type="l",col="red")
lines(forecast1.opt,type="l",col="blue")
legend("topleft",c("Actual Data",expression(paste(winter1)),
                   expression(paste(winter1.opt))),cex=0.5,
       col=c("black","red","blue"),lty=1)
```

#### Akurasi Data Latih

```{r}
#Akurasi data training
SSE1<-winter1$SSE
MSE1<-winter1$SSE/length(training.ts)
RMSE1<-sqrt(MSE1)
akurasi1 <- matrix(c(SSE1,MSE1,RMSE1))
row.names(akurasi1)<- c("SSE", "MSE", "RMSE")
colnames(akurasi1) <- c("Akurasi")
akurasi1

SSE1.opt<-winter1.opt$SSE
MSE1.opt<-winter1.opt$SSE/length(training.ts)
RMSE1.opt<-sqrt(MSE1.opt)
akurasi1.opt <- matrix(c(SSE1.opt,MSE1.opt,RMSE1.opt))
row.names(akurasi1.opt)<- c("SSE1.opt", "MSE1.opt", "RMSE1.opt")
colnames(akurasi1.opt) <- c("Akurasi")
akurasi1.opt

akurasi1.train = data.frame(Model_Winter = c("Winter 1","Winter1 optimal"),
                            Nilai_SSE=c(SSE1,SSE1.opt),
                            Nilai_MSE=c(MSE1,MSE1.opt),Nilai_RMSE=c(RMSE1,RMSE1.opt))
akurasi1.train
```

#### Akurasi Data Uji

```{r}
#Akurasi Data Testing
forecast1<-data.frame(forecast1)
testing.ts<-data.frame(testing.ts)
selisih1<-forecast1-testing.ts
SSEtesting1<-sum(selisih1^2)
MSEtesting1<-SSEtesting1/length(testing.ts)

forecast1.opt<-data.frame(forecast1.opt)
selisih1.opt<-forecast1.opt-testing.ts
SSEtesting1.opt<-sum(selisih1.opt^2)
MSEtesting1.opt<-SSEtesting1.opt/length(testing.ts)
```

### Winter Multiplikatif

Model multiplikatif digunakan cocok digunakan jika plot data asli menunjukkan fluktuasi musiman yang bervariasi.

#### Pemulusan

```{r}
#Pemulusan dengan winter multiplikatif 
winter2 <- HoltWinters(training.ts,alpha=0.2,beta=0.1,gamma=0.3,seasonal = "multiplicative")
winter2$fitted
xhat2 <- winter2$fitted[,2]

winter2.opt<- HoltWinters(training.ts, alpha= NULL,  beta = NULL, gamma = NULL, seasonal = "multiplicative")
winter2.opt$fitted
xhat2.opt <- winter2.opt$fitted[,2]
```

#### Peramalan

```{r}
#Forecast
forecast2 <- predict(winter2, n.ahead = 49)
forecast2.opt <- predict(winter2.opt, n.ahead = 49)
```

#### Plot Deret Waktu

```{r}
#Plot time series
plot(training.ts,main="Winter 0.2;0.1;0.1",type="l",col="black",
     xlim=c(1,25),pch=12)
lines(xhat2,type="l",col="red")
lines(xhat2.opt,type="l",col="blue")
lines(forecast2,type="l",col="red")
lines(forecast2.opt,type="l",col="blue")
legend("topleft",c("Actual Data",expression(paste(winter2)),
                   expression(paste(winter2.opt))),cex=0.5,
       col=c("black","red","blue"),lty=1)
```

#### Akurasi Data Latih

```{r}
#Akurasi data training
SSE2<-winter2$SSE
MSE2<-winter2$SSE/length(training.ts)
RMSE2<-sqrt(MSE2)
akurasi1 <- matrix(c(SSE2,MSE2,RMSE2))
row.names(akurasi1)<- c("SSE2", "MSE2", "RMSE2")
colnames(akurasi1) <- c("Akurasi lamda=0.2")
akurasi1

SSE2.opt<-winter2.opt$SSE
MSE2.opt<-winter2.opt$SSE/length(training.ts)
RMSE2.opt<-sqrt(MSE2.opt)
akurasi1.opt <- matrix(c(SSE2.opt,MSE2.opt,RMSE2.opt))
row.names(akurasi1.opt)<- c("SSE2.opt", "MSE2.opt", "RMSE2.opt")
colnames(akurasi1.opt) <- c("Akurasi")
akurasi1.opt

akurasi2.train = data.frame(Model_Winter = c("Winter 1","winter2 optimal"),
                            Nilai_SSE=c(SSE2,SSE2.opt),
                            Nilai_MSE=c(MSE2,MSE2.opt),Nilai_RMSE=c(RMSE2,RMSE2.opt))
akurasi2.train
```

#### Akurasi Data Uji

```{r}
#Akurasi Data Testing
forecast2<-data.frame(forecast2)
testing.ts<-data.frame(testing.ts)
selisih2<-forecast2-testing.ts
SSEtesting2<-sum(selisih2^2)
MSEtesting2<-SSEtesting2/length(testing.ts)

forecast2.opt<-data.frame(forecast2.opt)
selisih2.opt<-forecast2.opt-testing.ts
SSEtesting2.opt<-sum(selisih2.opt^2)
MSEtesting2.opt<-SSEtesting2.opt/length(testing.ts)
```

# Pertemuan 2

# Pemanggilan *Packages*

```{r}
library(dplyr)
library(TTR)
library(forecast)
library(lmtest) #digunakan untuk uji formal pendeteksian autokorelasi
library(orcutt) #untuk membuat model regresi Cochrane-Orcutt
library(HoRM) #untuk membuat model regresi Hildreth-Lu
```

# Input Data

Data yang digunakan dalam kesempatan kali ini adalah data IPM Provinsi Gorontalo periode tahun 2010-2021.

```{r}
tahun <- c(2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021)
ipm <- c(62.65,63.48,64.16,64.70,65.17,65.86,66.29,67.01,67.71,68.49,68.68,69.00)
dtGorontalo <- cbind(tahun,ipm)
dtGorontalo <- as.data.frame(dtGorontalo)
dtGorontalo
```

# Eksplorasi Data

Sebelum melakukan regresi, akan diperlihatkan *plot time-series* dari IPM Provinsi Gorontalo Periode 2010-2021

```{r}
#Membentuk objek time series
data.ts<-ts(dtGorontalo$ipm)
data.ts

#Membuat plot time series
ts.plot(data.ts, xlab="Time Period ", ylab="IPM", main= "Time Series Plot of IPM")
points(data.ts)
```

Selanjutnya akan dilakukan ramalan dan pemulusan dengan metode DMA dan DES karena terlihat pada plot di atas menunjukkan adanya *trend*.

```{r}
dt.sma <- SMA(data.ts, n=3)
dma <- SMA(dt.sma, n = 3)
At <- 2*dt.sma - dma
Bt <- 2/(3-1)*(dt.sma - dma)
dt.dma<- At+Bt
dt.ramal<- c(NA, dt.dma)

t = 1:5
f = c()

for (i in t) {
  f[i] = At[length(At)] + Bt[length(Bt)]*(i)
}
```

```{r}
dt.gab <- cbind(aktual = c(data.ts,rep(NA,5)), 
                pemulusan1 = c(dt.sma,rep(NA,5)),
                pemulusan2 = c(dt.dma, rep(NA,5)),
                At = c(At, rep(NA,5)), 
                Bt = c(Bt,rep(NA,5)),
                ramalan = c(dt.ramal, f[-1]))
dt.gab

#Plot time series
ts.plot(dt.gab[,1], xlab="Time Period ", ylab="IPM", 
        main= "DMA N=3 Data IPM", ylim=c(62,75))
points(dt.gab[,1])
points(dt.gab[,3])
points(dt.gab[,6])
lines(dt.gab[,3],col="green",lwd=2)
lines(dt.gab[,6],col="red",lwd=2)
legend("topleft",c("data aktual","data pemulusan","data peramalan"), 
       lty=8, col=c("black","green","red"), cex=0.8)
```

Selanjutnya akan dilihat keakuratan dari metode DMA

```{r}
#Menghitung nilai keakuratan
error.dma = data.ts-dt.ramal[1:length(data.ts)]
SSE.dma = sum(error.dma[6:length(data.ts)]^2)
MSE.dma = mean(error.dma[6:length(data.ts)]^2)
MAPE.dma = mean(abs((error.dma[6:length(data.ts)]/data.ts[6:length(data.ts)])*100))

akurasi.dma <- matrix(c(SSE.dma, MSE.dma, MAPE.dma))
row.names(akurasi.dma)<- c("SSE", "MSE", "MAPE")
colnames(akurasi.dma) <- c("Akurasi m = 3")
akurasi.dma
```

Selanjutnya akan digunakan metode *Double Exponential Smoothing* dengan cara sebagai berikut.

Pertama akan data akan dibagi menjadi data *training* dan data *testing*.

```{r}
#membagi training dan testing
training<-dtGorontalo[1:10,2]
testing<-dtGorontalo[11:12,2]

#data time series
training.ts<-ts(training)
testing.ts<-ts(testing,start=11)

#eksplorasi data
plot(data.ts, col="red",main="Plot semua data")
points(data.ts)

plot(training.ts, col="blue",main="Plot data training")
points(training.ts)
```

Selanjutnya akan dilakukan pemulusan dengan DES, kali ini langsung dicari lambda dan gamma optimum sebagai berikut. Nilai lambda dan gamma optimum dapat dilihat pada smoothing parameters alpha untuk nilai lambda dan beta untuk nilai gamma.

```{r}
#Lamda dan gamma optimum
des.opt<- HoltWinters(training.ts, gamma = FALSE)
des.opt
plot(des.opt)
legend("topleft", c("Data Aktual", "Peramalan"), col = c("black", "red"), 
       lty = c(1,1))


#ramalan
ramalandesopt<- forecast(des.opt, h=5)
ramalandesopt
```

Selanjutnya akan dicari akurasi dari metode DES.

```{r}
ssedes.train<-des.opt$SSE
msedes.train<-ssedes.train/length(training.ts)
sisaandes<-ramalandesopt$residuals
head(sisaandes)

mapedes.train <- sum(abs(sisaandes[3:length(training.ts)]/training.ts[3:length(training.ts)])*100)/length(training.ts)

akurasides.opt <- matrix(c(ssedes.train,msedes.train,mapedes.train))
row.names(akurasides.opt)<- c("SSE", "MSE", "MAPE")
colnames(akurasides.opt) <- c("Akurasi lamda dan gamma optimum")
akurasides.opt
```

```{r}
#Akurasi data testing
selisihdesopt<-ramalandesopt$mean-testing.ts
selisihdesopt

SSEtestingdesopt<-sum(selisihdesopt^2)
SSEtestingdesopt<-SSEtestingdesopt/length(testing.ts)
MAPEtestingdesopt<-sum(abs(selisihdesopt/testing.ts)*100)/length(testing.ts)

akurasiDesTesting <- matrix(c(SSEtestingdesopt,SSEtestingdesopt,MAPEtestingdesopt))
row.names(akurasiDesTesting)<- c("SSE", "MSE", "MAPE")
colnames(akurasiDesTesting) <- c("Akurasi lamda dan gamma optimum")
akurasiDesTesting
```

Setelah didapatkan nilai akurasi untuk metode DMA dan DES, selanjutnya akan dibandingkan keakuratan antar metode keduanya.

```{r}
cbind(akurasi.dma, akurasides.opt)
```

Berdasarkan perbandingan akurasi tersebut, terlihat nilai SSE, MSE, dan MAPE metode DES lebih kecil dibandingkan dengan metode DMA. Oleh karena itu, metode peramalan dan pemulusan yang terbaik antara keduanya adalah dengan metode DES.

Setelah melakukan peramalan, data yang telah dimasukkan kemudian dieksplorasi. Eksplorasi pertama yang dilakukan adalah dengan menggunakan *scatter plot*.

```{r}
#Eksplorasi Data
#Pembuatan Scatter Plot
plot(tahun,ipm, pch = 20, col = "blue",
     main = "Scatter Plot Tahun vs Nilai IPM",
     xlab = "Tahun",
     ylab = "Nilai IPM")
#Menampilkan Nilai Korelasi
cor(tahun,ipm)
```

Berdasarkan scatter plot di atas, terlihat adanya hubungan / korelasi positif antara peubah tahun dengan nilai IPM, terlihat titik-titik pada plot yang naik ke arah kanan atas. Hal tersebut juga diperkuat dengan hasil perhitungan aplikasi `R` di mana didapatkan nilai korelasi sebesar $0.9966848$.

Setalah mengetahui adanya hubungan antar dua peubah, maka model regresi dapat ditentukan.

# Regresi

```{r}
#Pembuatan Model Regresi
#model regresi
model<- lm(ipm~tahun, data = dtGorontalo)
summary(model)
```

Model yang dihasilkan adalah $$y_i=-1118+0.5873x_t$$ Berdasarkan ringkasan model dapat diketahui bahwa hasil uji F memiliki *p-value* \< $\alpha$ (5%). Artinya, minimal terdapat satu variabel yang berpengaruh nyata terhadap model. Hasil uji-t parsial kedua parameter regresi, yaitu intersep dan koefisien regresi juga menunjukkan hal yang sama, yaitu memiliki *p-value* \< $\alpha$ (5%) sehingga nyata dalam taraf 5%. Selanjutnya dapat dilihat juga nilai $R^2=0.9934$. Artinya, sebesar 99.34% keragaman nilai IPM dapat dijelaskan oleh peubah tahun. Hasil ini menunjukkan hasil yang bagus, seolah mendapatkan hasil terbaik. Namun, kita perlu melakukan uji terhadap sisaannya seperti berikut ini.

```{r}
#sisaan dan fitted value
sisaan<- residuals(model)
fitValue<- predict(model)

#Diagnostik dengan eksploratif
par(mfrow = c(2,2))
qqnorm(sisaan)
qqline(sisaan, col = "steelblue", lwd = 2)
plot(fitValue, sisaan, col = "steelblue", pch = 20, xlab = "Sisaan", ylab = "Fitted Values", main = "Sisaan vs Fitted Values")
abline(a = 0, b = 0, lwd = 2)
hist(sisaan, col = "steelblue")
plot(seq(1,12,1), sisaan, col = "steelblue", pch = 20, xlab = "Sisaan", ylab = "Order", main = "Sisaan vs Order")
lines(seq(1,12,1), sisaan, col = "red")
abline(a = 0, b = 0, lwd = 2)
```

Dua plot di samping kiri digunakan untuk melihat apakah sisaan menyebar normal. Normal Q-Q Plot di atas menunjukkan bahwa sisaan cenderung menyebar normal, tetapi histogram dari sisaan tidak menunjukkan demikian. Selanjutnya, dua plot di samping kanan digunakan untuk melihat autokorelasi. Plot Sisaan vs *Fitted Value* dan Plot Sisaan vs *Order* menunjukkan adanya pola pada sisaan. Untuk lebih lanjut akan digunakan uji formal melihat normalitas sisaan dan plot ACF dan PACF untuk melihat apakah ada autokorelasi atau tidak.

```{r}
#Melihat Sisaan Menyebar Normal/Tidak
#H0: sisaan mengikuti sebaran normal
#H1: sisaan tidak mengikuti sebaran normal
shapiro.test(sisaan)
ks.test(sisaan, "pnorm", mean=mean(sisaan), sd=sd(sisaan))
```

Berdasarkan uji formal Saphiro-Wilk dan Kolmogorov-Smirnov didapatkan nilai *p-value* \> $\alpha$ (5%). Artinya, cukup bukti untuk menyatakan sisaan berdistribusi normal.

```{r}
#ACF dan PACF identifikasi autokorelasi
par(mfrow = c(1,2))
acf(sisaan)
pacf(sisaan)
```

Berdasarkan plot ACF dan PACF, terlihat semua dalam rentang batas dan tidak ada lag yang signifikan. Namun, untuk lebih memastikan akan dilakukan uji formal dengan uji Durbin Watson.

```{r}
#Deteksi autokorelasi dengan uji-Durbin Watson
#H0: tidak ada autokorelasi
#H1: ada autokorelasi
dwtest(model)
```

Berdasarkan hasil DW Test, didapatkan nilai $DW = 1.2644$ dan *p-value* = $0.03741$. Berdasarkan tabel Durbin-Watson diperoleh nilai $DL = 0.9771$ dan $DU = 1.331$. Nilai DW masih berada di antara nilai DL dan DU. Artinya, berada di daerah inkonklusif, tidak dapat dikatakan berada di daerah autokorelasi positif maupun bebas dari autokorelasi. Namun, dengan nilai *p-value* \< 0.05 dapat disimpulkan bahwa tolak H0, cukup bukti mengatakan adanya autokorelasi. Oleh karena itu, diperlukan penangan autokorelasi. Penanganan yang akan digunakan menggunakan dua metode, yaitu Cochrane-Orcutt dan Hildret-Lu.

# Penanganan Autokorelasi

## Metode Cochrane-Orcutt

Penanganan metode Cochrane-Orcutt dapat dilakukan dengan bantuan packages Orcutt pada aplikasi `R` maupun secara manual. Berikut ini ditampilkan cara menggunakan bantuan `library` *packages* `Orcutt`.

```{r}
#Penanganan Autokorelasi Cochrane-Orcutt
modelCO<-cochrane.orcutt(model)
modelCO
```

Hasil keluaran model setelah dilakukan penanganan adalah sebagai berikut. $$y_i=-1062.042646+0.559755x_t$$ Hasil juga menunjukkan bahwa nilai DW dan p-value meningkat menjadi $1.49014$ dan $0.09403$. Nilai DW sudah berada pada rentang DU \< DW \< 4-DU atau $1.331 < DW < 2.669$. Hal tersebut juga didukung dengan nilai *p-value* \> 0.05, artinya belum cukup bukti menyatakan bahwa sisaan terdapat autokorelasi pada taraf nyata 5%. Untuk nilai $ρ ̂$ optimum yang digunakan adalah $0.3409214$. Nilai tersebut dapat diketahui dengan *syntax* berikut.

```{r}
#Rho optimum
rho<- modelCO$rho
rho
```

Selanjutnya akan dilakukan transformasi secara manual dengan syntax berikut ini.

```{r}
ipm
```

```{r}
ipm[-1]
```

```{r}
#Transformasi Manual
ipm.trans<- ipm[-1]-ipm[-12]*rho
tahun.trans<- tahun[-1]-tahun[-12]*rho
modelCOmanual<- lm(ipm.trans~tahun.trans)
summary(modelCOmanual)
```

Hasil model transformasi bukan merupakan model sesungguhnya. Koefisien regresi masih perlu dicari kembali mengikuti $β_0^*=β_0+ρ ̂β_0$ dan $β_1^*=β_1$.

```{r}
#Mencari Penduga Koefisien Regresi setelah Transformasi ke Persamaan Awal
b0bintang <- modelCOmanual$coefficients[-2]
b0 <- b0bintang/(1-rho)
b1 <- modelCOmanual$coefficients[-1]
b0
b1
```

Hasil perhitungan koefisien regresi tersebut akan menghasilkan hasil yang sama dengan model yang dihasilkan menggunakan *packages*.

## Metode Hildreth-Lu

Penanganan kedua adalah menggunakan metode Hildreth-Lu. Metode ini akan mencari nilai SSE terkecil dan dapat dicari secara manual maupun menggunakan packages. Jika menggunakan packages, gunakan `library` *packages* `HORM`.

```{r}
#Penanganan Autokorelasi Hildreth lu
# Hildreth-Lu
hildreth.lu.func<- function(r, model){
  x <- model.matrix(model)[,-1]
  y <- model.response(model.frame(model))
  n <- length(y)
  t <- 2:n
  y <- y[t]-r*y[t-1]
  x <- x[t]-r*x[t-1]
  
  return(lm(y~x))
}

#Pencariab rho yang meminimumkan SSE
r <- c(seq(0.1,0.9, by= 0.1))
tab <- data.frame("rho" = r, "SSE" = sapply(r, function(i){deviance(hildreth.lu.func(i, model))}))
round(tab, 4)
```

Pertama-tama akan dicari di mana kira-kira $ρ$ yang menghasilkan SSE minimum. Pada hasil di atas terlihat $ρ$ minimum ketika 0.3. Namun, hasil tersebut masih kurang teliti sehingga akan dicari kembali $ρ$ yang lebih optimum dengan ketelitian yang lebih. Jika sebelumnya jarak antar $ρ$ yang dicari adalah 0.1, kali ini jarak antar $ρ$ adalah 0.001 dan dilakukan pada selang 0.2 sampai dengan 0.5.

```{r}
#Rho optimal di sekitar 0.4
rOpt <- seq(0.2,0.5, by= 0.001)
tabOpt <- data.frame("rho" = rOpt, "SSE" = sapply(rOpt, function(i){deviance(hildreth.lu.func(i, model))}))
head(tabOpt[order(tabOpt$SSE),])

#Grafik SSE optimum
par(mfrow = c(1,1))
plot(tab$SSE ~ tab$rho , type = "l", xlab = "Rho", ylab = "SSE")
abline(v = tabOpt[tabOpt$SSE==min(tabOpt$SSE),"rho"], lty = 2, col="red",lwd=2)
text(x=0.341, y=0.2397500, labels = "rho=0.341", cex = 0.8)
```

Perhitungan yang dilakukan aplikasi `R` menunjukkan bahwa nilai $ρ$ optimum, yaitu saat SSE terkecil terdapat pada nilai $ρ=0.341$. Hal tersebut juga ditunjukkan pada plot. Selanjutnya, model dapat didapatkan dengan mengevaluasi nilai $ρ$ ke dalam fungsi `hildreth.lu.func`, serta dilanjutkan dengan pengujian autokorelasi dengan uji Durbin-Watson. Namun, setelah pengecekan tersebut tidak lupa koefisien regresi tersebut digunakan untuk transformasi balik. Persamaan hasil transformasi itulah yang menjadi persamaan sesungguhnya.

```{r}
#Model terbaik
modelHL <- hildreth.lu.func(0.341, model)
summary(modelHL)

#Transformasi Balik
cat("y = ", coef(modelHL)[1]/(1-0.341), "+", coef(modelHL)[2],"x", sep = "")
```

Setelah dilakukan tranformasi balik, didapatkan model dengan metode Hildreth-Lu sebagai berikut. $$y_i=-1062.032+0.5597492x_t$$

```{r}
#Deteksi autokorelasi
dwtest(modelHL)
```

Hasil uji Durbin-Watson juga menunjukkan bawah nilai DW sebesar $1.4092$ berada pada selang daerah tidak ada autokorelasi, yaitu pada rentang DU \< DW \< 4-DU atau $1.331 < DW < 2.669$. Hal tersebut juga didukung oleh *p-value* sebesar $0.09404$, di mana *p-value* \> $\alpha$=5%. Artinya tak tolak $H_0$ atau belum cukup bukti menyatakan bahwa ada autokorelasi dalam data nilai IPM dengan metode Hildreth-Lu pada taraf nyata 5%.

Terakhir, akan dibandingkan nilai SSE dari ketiga metode (metode awal, metode Cochrane-Orcutt, dan Hildreth-Lu).

```{r}
#Perbandingan
sseModelawal <- anova(model)$`Sum Sq`[-1]
sseModelCO <- anova(modelCOmanual)$`Sum Sq`[-1]
sseModelHL <- anova(modelHL)$`Sum Sq`[-1]
mseModelawal <- sseModelawal/length(ipm)
mseModelCO <- sseModelCO/length(ipm)
mseModelHL <- sseModelHL/length(ipm)
akurasi <- matrix(c(sseModelawal,sseModelCO,sseModelHL,
                    mseModelawal,mseModelCO,mseModelHL),nrow=2,ncol=3,byrow = T)
colnames(akurasi) <- c("Model Awal", "Model Cochrane-Orcutt", "Model Hildreth-Lu")
row.names(akurasi) <- c("SSE","MSE")
akurasi
```

Berdasarkan hasil tersebut dapat diketahui bahwa hasil penanganan autokorelasi dengan metode Cochrane-Orcutt dan Hildreth-Lu memiliki SSE yang sama, sebesar $0.23975$ dan lebih baik dibandingkan model awal ketika autokorelasi masih terjadi, yaitu sebesar $0.3286364$.

# Simpulan

Autokorelasi yang terdapat pada data IPM terjadi akibat adanya korelasi di antara unsur penyusunnya. Indikator IPM yang erat hubungannya dengan perekonomian sangat rawan menjadi penyebab adanya autokorelasi. Adanya autokorelasi menyebabkan model regresi kurang baik karena akan meingkatkan galatnya. Autokorelasi dapat dideteksi secara eksploratif melalui plot sisaan, ACF, dan PACF, serta dengan uji formal Durbin-Watson. Namun, autokorelasi tersebut dapat ditangani dengan metode Cochrane-Orcutt dan Hildreth-Lu. Kedua metode menghasilkan nilai SSE yang sama, artinya keduanya baik untuk digunakan.

# Pertemuan 3
---
title: "Pertemuan 3 - Regresi dengan Peubah Lag"
author:
- Adelia Putri Pangestika
- Muhammad Rizky Nurhambali
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
  word_document: default
  pdf_document: default
---

## *Packages*

```{r, echo=FALSE}
library(dLagM)
library(dynlm)
library(MLmetrics)
library(lmtest)
library(car)
```

## Impor Data

```{r}
data <- rio::import("https://raw.githubusercontent.com/rizkynurhambali/Praktikum-MPDW-2324/main/Pertemuan%203/Data%20Asli.csv")
str(data)
data
```

## Pembagian Data

```{r}
#SPLIT DATA
train<-data[1:15,]
test<-data[16:20,]
```

```{r}
#data time series
train.ts<-ts(train)
test.ts<-ts(test)
data.ts<-ts(data)
```

## Model Koyck

Model Koyck didasarkan pada asumsi bahwa semakin jauh jarak lag peubah independen dari periode sekarang maka semakin kecil pengaruh peubah lag terhadap peubah dependen.

Koyck mengusulkan suatu metode untuk menduga model dinamis distributed lag dengan mengasumsikan bahwa semua koefisien $\beta$ mempunyai tanda sama.

Model kyock merupakan jenis paling umum dari model infinite distributed lag dan juga dikenal sebagai geometric lag

$$
y_t=a(1-\lambda)+\beta_0X_t+\beta_1Z_t+\lambda Y_{t-1}+V_t
$$

dengan $$V_t=u_t-\lambda u_{t-1}$$

### Pemodelan

Pemodelan model Koyck dengan `R` dapat menggunakan `dLagM::koyckDlm()` . Fungsi umum dari `koyckDlm` adalah sebagai berikut.

```{r, eval=FALSE, message = FALSE, warning=FALSE, error=FALSE}
koyckDlm(x , y , intercept)
```

Fungsi `koyckDlm()` akan menerapkan model lag terdistribusi dengan transformasi Koyck satu prediktor. Nilai `x` dan `y` tidak perlu sebagai objek *time series* (`ts`). `intercept` dapat dibuat `TRUE` untuk memasukkan intersep ke dalam model.

```{r}
#MODEL KOYCK
model.koyck <- koyckDlm(x = train$Xt, y = train$Yt)
summary(model.koyck)
AIC(model.koyck)
BIC(model.koyck)
```

Dari hasil tersebut, didapat bahwa peubah $x_t$ dan $y_{t-1}$ memiliki nilai $P-Value<0.05$. Hal ini menunjukkan bahwa peubah $x_t$ dan $y_{t-1}$ berpengaruh signifikan terhadap $y$. Adapun model keseluruhannya adalah sebagai berikut

$$
\hat{Y_t}=-3.7335+1.1510X_t+0.4214Y_{t-1}
$$

### Peramalan dan Akurasi

Berikut adalah hasil peramalan y untuk 5 periode kedepan menggunakan model koyck

```{r}
fore.koyck <- forecast(model = model.koyck, x=test$Xt, h=5)
fore.koyck
mape.koyck <- MAPE(fore.koyck$forecasts, test$Yt)
#akurasi data training
GoF(model.koyck)
```

## Regression with Distributed Lag

Pemodelan model Regression with Distributed Lag dengan `R` dapat menggunakan `dLagM::dlm()` . Fungsi umum dari `dlm` adalah sebagai berikut.

```{r, eval=FALSE, error=FALSE}
dlm(formula , data , x , y , q , remove )
```

Fungsi `dlm()` akan menerapkan model lag terdistribusi dengan satu atau lebih prediktor. Nilai `x` dan `y` tidak perlu sebagai objek *time series* (`ts`). $q$ adalah integer yang mewakili panjang *lag* yang terbatas.

### Pemodelan (Lag=2)

```{r}
model.dlm <- dlm(x = train$Xt,y = train$Yt , q = 2)
summary(model.dlm)
AIC(model.dlm)
BIC(model.dlm)
```

Dari hasil diatas, didapat bahwa $P-value$ dari intercept dan $x_{t-1}<0.05$. Hal ini menunjukkan bahwa intercept dan $x_{t-1}$ berpengaruh signifikan terhadap $y$. Adapun model keseluruhan yang terbentuk adalah sebagai berikut

$$
\hat{Y_t}=-9.6779+0.3179X_t+1.5276X_{t-1}+0.2651X_{t-2}
$$

### Peramalan dan Akurasi

Berikut merupakan hasil peramalan $y$ untuk 5 periode kedepan

```{r}
fore.dlm <- forecast(model = model.dlm, x=test$Xt, h=5)
fore.dlm
mape.dlm <- MAPE(fore.dlm$forecasts, test$Yt)
#akurasi data training
GoF(model.dlm)
```

### *Lag* Optimum

```{r}
#penentuan lag optimum 
finiteDLMauto(formula = Yt ~ Xt,
              data = data.frame(train), q.min = 1, q.max = 6,
              model.type = "dlm", error.type = "AIC", trace = FALSE)
```

Berdasarkan output tersebut, lag optimum didapatkan ketika lag=6. Selanjutnya dilakukan pemodelan untuk lag=6

```{r}
#model dlm dengan lag optimum
model.dlm2 <- dlm(x = train$Xt,y = train$Yt , q = 6)
summary(model.dlm2)
AIC(model.dlm2)
BIC(model.dlm2)
```

Dari hasil tersebut terdapat beberapa peubah yang berpengaruh signifikan terhadap taraf nyata 5% yaitu $x_t$ , $x_{t-2}$ , $x_{t-4}$ , $x_{t-6}$. Adapun keseluruhan model yang terbentuk adalah

$$
\hat{Y_t}=21.42223+1.68749X_t+...-3.47612X_{t-6}
$$

Adapun hasil peramalan 5 periode kedepan menggunakan model tersebut adalah sebagai berikut

```{r}
#peramalan dan akurasi
fore.dlm2 <- forecast(model = model.dlm2, x=test$Xt, h=5)
mape.dlm2<- MAPE(fore.dlm2$forecasts, test$Yt)
#akurasi data training
GoF(model.dlm2)
```

Model tersebut merupakan model yang sangat baik dengan nilai MAPE yang kurang dari 10%.

## Model Autoregressive

Peubah dependen dipengaruhi oleh peubah independen pada waktu sekarang, serta dipengaruhi juga oleh peubah dependen itu sendiri pada satu waktu yang lalu maka model tersebut disebut *autoregressive* (Gujarati 2004).

### Pemodelan

Pemodelan Autoregressive dilakukan menggunakan fungsi `dLagM::ardlDlm()` . Fungsi tersebut akan menerapkan *autoregressive* berordo $(p,q)$ dengan satu prediktor. Fungsi umum dari `ardlDlm()` adalah sebagai berikut.

```{r, eval=FALSE}
ardlDlm(formula = NULL , data = NULL , x = NULL , y = NULL , p = 1 , q = 1 , 
         remove = NULL )
```

Dengan $p$ adalah integer yang mewakili panjang *lag* yang terbatas dan $q$ adalah integer yang merepresentasikan ordo dari proses *autoregressive*.

```{r}
#model.ardl <- ardlDlm(x = train$Xt, y = train$Yt, p = 1 , q = 1)
#summary(model.ardl)
#AIC(model.ardl)
#BIC(model.ardl)
```

```{r}
model.ardl <- ardlDlm(formula = Yt ~ Xt, 
                         data = train,p = 1 , q = 1)
summary(model.ardl)
AIC(model.ardl)
BIC(model.ardl)
```

Hasil di atas menunjukkan bahwa selain peubah $x_{t-1}$, hasil uji t menunjukkan nilai-p pada peubah $\ge0.05$ Hal ini menunjukkan bahwa peubah $x_{t-1}$ berpengaruh signifikan terhadap $y_t$, sementara $x_t$ dan $y_{t-1}$ berpengaruh signifikan terhadap $y_t$. Model keseluruhannya adalah sebagai berikut:

$$
\hat{Y}=-8,3594+0,3563X_t+1,4557X_{t-1}+0,1408Y_{t-1}
$$

### Peramalan dan Akurasi

```{r}
fore.ardl <- forecast(model = model.ardl, x=test$Xt, h=5)
fore.ardl
```

Data di atas merupakan hasil peramalan untuk 5 periode ke depan menggunakan Model Autoregressive dengan $p=1$ dan $q=1$.

```{r}
mape.ardl <- MAPE(fore.ardl$forecasts, test$Yt)
mape.ardl
#akurasi data training
GoF(model.ardl)
```

Berdasarkan akurasi di atas, terlihat bahwa nilai MAPE keduanya tidak jauh berbeda. Artinya, model regresi dengan distribusi lag ini tidak `overfitted` atau `underfitted`

### *Lag* Optimum

```{r}
#penentuan lag optimum
model.ardl.opt <- ardlBoundOrders(data = data.frame(data), ic = "AIC", 
                                  formula = Yt ~ Xt )
min_p=c()
for(i in 1:6){
  min_p[i]=min(model.ardl.opt$Stat.table[[i]])
}
q_opt=which(min_p==min(min_p, na.rm = TRUE))
p_opt=which(model.ardl.opt$Stat.table[[q_opt]] == 
              min(model.ardl.opt$Stat.table[[q_opt]], na.rm = TRUE))
data.frame("q_optimum" = q_opt, "p_optimum" = p_opt, 
           "AIC"=model.ardl.opt$min.Stat)
```

Dari tabel di atas, dapat terlihat bahwa nilai AIC terendah didapat ketika $p=6$ dan $q=1$, yaitu sebesar `-20,56587`. Artinya, model autoregressive optimum didapat ketika $p=6$ dan $q=1$.

Selanjutnya dapat dilakukan pemodelan dengan nilai $p$ dan $q$ optimum seperti inisialisasi di langkah sebelumnya.

## Pemodelan DLM & ARDL dengan Library `dynlm`

Pemodelan regresi dengan peubah *lag* tidak hanya dapat dilakukan dengan fungsi pada *packages* `dLagM` , tetapi terdapat *packages* `dynlm` yang dapat digunakan. Fungsi `dynlm` secara umum adalah sebagai berikut.

```{r, eval=FALSE}
dynlm(formula, data, subset, weights, na.action, method = "qr",
  model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,
  contrasts = NULL, offset, start = NULL, end = NULL, ...)
```

Untuk menentukan `formula` model yang akan digunakan, tersedia fungsi tambahan yang memungkinkan spesifikasi dinamika (melalui `d()` dan `L()`) atau pola linier/siklus dengan mudah (melalui `trend()`, `season()`, dan `harmon()`). Semua fungsi formula baru mengharuskan argumennya berupa objek deret waktu (yaitu, `"ts"` atau `"zoo"`).

```{r}
#sama dengan model dlm q=1
cons_lm1 <- dynlm(Yt ~ Xt+L(Xt),data = train.ts)
#sama dengan model ardl p=1 q=0
cons_lm2 <- dynlm(Yt ~ Xt+L(Yt),data = train.ts)
#sama dengan ardl p=1 q=1
cons_lm3 <- dynlm(Yt ~ Xt+L(Xt)+L(Yt),data = train.ts)
#sama dengan dlm p=2
cons_lm4 <- dynlm(Yt ~ Xt+L(Xt)+L(Xt,2),data = train.ts)
```

### Ringkasan Model

```{r}
summary(cons_lm1)
summary(cons_lm2)
summary(cons_lm3)
summary(cons_lm4)
```

### SSE

```{r}
deviance(cons_lm1)
deviance(cons_lm2)
deviance(cons_lm3)
deviance(cons_lm4)
```

### Uji Diagnostik

```{r}
#uji model
if(require("lmtest")) encomptest(cons_lm1, cons_lm2)
```

#### Autokorelasi

```{r}
#durbin watson
dwtest(cons_lm1)
dwtest(cons_lm2)
dwtest(cons_lm3)
dwtest(cons_lm4)
```

#### Heterogenitas

```{r}
bptest(cons_lm1)
bptest(cons_lm2)
bptest(cons_lm3)
bptest(cons_lm4)
```

#### Kenormalan

```{r}
shapiro.test(residuals(cons_lm1))
shapiro.test(residuals(cons_lm2))
shapiro.test(residuals(cons_lm3))
shapiro.test(residuals(cons_lm4))
```

## Perbandingan Model

```{r}
akurasi <- matrix(c(mape.koyck, mape.dlm, mape.dlm2, mape.ardl))
row.names(akurasi)<- c("Koyck","DLM 1","DLM 2","Autoregressive")
colnames(akurasi) <- c("MAPE")
akurasi
```

Berdasarkan nilai MAPE, model paling optimum didapat pada Model Koyck karena memiliki nilai MAPE yang terkecil.

### Plot

```{r}
par(mfrow=c(1,1))
plot(test$Xt, test$Yt, type="b", col="black", ylim=c(120,250))
points(test$Xt, fore.koyck$forecasts,col="red")
lines(test$Xt, fore.koyck$forecasts,col="red")
points(test$Xt, fore.dlm$forecasts,col="blue")
lines(test$Xt, fore.dlm$forecasts,col="blue")
points(test$Xt, fore.dlm2$forecasts,col="orange")
lines(test$Xt, fore.dlm2$forecasts,col="orange")
points(test$Xt, fore.ardl$forecasts,col="green")
lines(test$Xt, fore.ardl$forecasts,col="green")
legend("topleft",c("aktual", "koyck","DLM 1","DLM 2", "autoregressive"), lty=1, col=c("black","red","blue","orange","green"), cex=0.8)
```

Berdasarkan plot tersebut, terlihat bahwa plot yang paling mendekati data aktualnya adalah Model koyck, sehingga dapat disimpulkan model terbaik dalam hal ini adalah model regresi koyck

## Pengayaan (Regresi Berganda)

### Data

```{r}
data(M1Germany)
data1 = M1Germany[1:144,]
```

### DLM

```{r}
#Run the search over finite DLMs according to AIC values
finiteDLMauto(formula = logprice ~ interest+logm1,
              data = data.frame(data1), q.min = 1, q.max = 5,
              model.type = "dlm", error.type = "AIC", trace = FALSE)
```

```{r}
#model dlm berganda
model.dlmberganda = dlm(formula = logprice ~ interest + logm1,
                data = data.frame(data1) , q = 5)
summary(model.dlmberganda)

model.dlmberganda2 = dlm(formula = logprice ~ interest + logm1,
                        data = data.frame(data1) , q = 1)
summary(model.dlmberganda2)
```

### ARDL

```{r}
#Mencari orde lag optimum model ARDL
ardlBoundOrders(data = data1 , formula = logprice ~ interest + logm1,
                ic="AIC")

model.ardlDlmberganda = ardlDlm(formula = logprice ~ interest + logm1,
                        data = data.frame(data1) , p = 4 , q = 4)
summary(model.ardlDlmberganda)
```

```{r}
#model p interest 0 p logm1 4 
rem.p = list(interest = c(1,2,3,4))
remove = list(p = rem.p)
model.ardlDlmberganda2 = ardlDlm(formula = logprice ~ interest + logm1,
                        data = data.frame(data1) , p = 4 , q = 4 ,
                        remove = remove)
summary(model.ardlDlmberganda2)
```

Proses selanjutnya sama dengan pemodelan menggunakan peubah tunggal.

# Pertemuan 4

## White Noise

Pembangkitan data berpola AR, MA, ARMA, dan banyak proses deret waktu lainnya diawali pembangkitan *white noise*. *White noise* merupakan sederet nilai dari peubah bebas stokastik identik. Oleh karena itu, *white noise* memiliki dua karakteristik penting:

1.  *White noise* tidak memiliki autokorelasi (**karena saling bebas**)
2.  Nilai harapan dan ragam *white noise* sama (**karena berasal dari peubah acak bebas stokastik identik**)

*White noise* dibangkitkan dari suatu peubah acak, umumnya peubah acak normal.

```{r}
wn <- rnorm(200)
ts.plot(wn)
```

Dapat terlihat bahwa *white noise* tidak memiliki autokorelasi dari ACF. Perhatikan bahwa lag ke-0 adalah korelasi observasi ke-t dengan dirinya sendiri. Nilai korelasi tersebut pasti 1. Sebagai alternatif, lag pertama di plot ACF dapat ditetapkan sebagai 1 (alih-alih 0) dengan menambahkan argumen `xlim(1, lag akhir)`. Plot tersebut dapat disandingkan bersamaan dengan membuat matriks $1 \times 2$ dengan `par(mfrow = c(1,2))`.

```{r}
par(mfrow = c(1, 2)) 
acf(wn)
acf(wn, xlim = c(1, 20))
```

## Proses MA

Proses MA dapat dituliskan sebagai berikut:

$$
y_{t} = c + e_t + \theta_{1}e_{t-1} + \theta_{2}e_{t-2} + \dots + \theta_{q}e_{t-q} = c+{e_t+\sum_{i=1}^p \theta_ie_{t-i}}
$$ Terlihat bahwa $e_t$, atau *white noise*, berperan penting dalam pembangkitan proses MA.

## Pembangkitan Proses MA(1)

Akan dicoba membangkitkan proses MA paling sederhana, yaitu MA(1) dengan $\theta = 0.5$ sebanyak 200 observasi dan $c=0$. Karena diperlukan satu nilai awal untuk $e_{t-1}$, masukkan nilai pertama white noise sebagai nilai awal tersebut.

```{r}
set.seed(1234)
ma <- wn[1]
```

Nilai-nilai selanjutnya dapat dicari melalui *loop*. Bentuk loop dapat dilihat dari rumus MA(1) yang hendak dibangkitkan:

$$
y_t = e_t+0.5e_{t-1}
$$

```{r}
for(i in 2:200){
   ma[i] <- wn[i] + 0.5 * wn[i - 1] 
}
ma
```

Selain menggunakan cara di atas, pembangkitan proses MA(1) dapat dilakukan dengan fungsi `arima.sim()` sebagai berikut.

```{r}
ma1 <- arima.sim(list(order=c(0,0,1), ma=0.5), n=200)
ma1
```

## Karakteristik MA(1)

### Plot Time Series

```{r}
ts.plot(ma)
```

Berdasarkan plot time series, terlihat bahwa data MA(1) yang dibangkitkan stasioner dalam rataan

### Plot ACF

```{r}
acf(ma,lag.max = 20)
```

Berdasarkan plot AFC tersebut, terlihat bahwa plot ACF *cuts off* di lag pertama

### Plot PACF

```{r}
pacf(ma)
```

Berdasarkan plot PACF tersebut, terlihat bahwa plot PACF cenderung *tails off* dan membentuk gelombang sinus

### Plot EACF

```{r}
TSA::eacf(ma)
```

Berdasarkan pola segitiga nol pada plot EACF, terlihat bahwa segitiga nol berada pada ordo AR(0) dan ordo MA(1)

### Scatterplot Antar Lag

#### Korelasi antara $Y_t$ dengan $Y_{t-1}$

```{r}
#Yt
yt_ma <- ma[-1]
yt_ma
#Yt-1
yt_1_ma <- ma[-200]
yt_1_ma
```

```{r}
plot(y=yt_ma,x=yt_1_ma)
```

Berdasarkan scatterplot tersebut, terlihat bahwa terdapat hubungan positif antara $Y_t$ dengan $Y_{t-1}$. Hal ini sesuai dengan teori yang ada

```{r}
cor(yt_ma,yt_1_ma)
```

Korelasi antara $Y_t$ dengan $Y_{t-1}$ dari hasil simulasi mendekati perhitungan teoritis yaitu

$$
\rho_1=\frac{-\theta}{1+(-\theta)^2}=\frac{-(-0.5)}{1+(-0.5)^2}=0.4
$$

#### Korelasi antara $Y_t$ dengan $Y_{t-2}$

```{r}
#Yt
yt_ma2 <- ma[-c(1,2)]
yt_ma2
#Yt-2
yt_2_ma <- ma[-c(199,200)]
yt_2_ma
```

```{r}
plot(y=yt_ma2,x=yt_2_ma)
```

Berdasarkan scatterplot tersebut, terlihat bahwa cenderung tidak terdapat hubungan antara $Y_t$ dengan $Y_{t-2}$.

```{r}
cor(yt_ma2,yt_2_ma)
```

Korelasi antara $Y_t$ dengan $Y_{t-2}$ hasil simulasi mendekati teori yang ada yaitu 0.

## Proses AR

Proses AR dapat dituliskan sebagai berikut:

$$ y_{t} = c + e_t + \phi_{1}Y_{t-1} + \phi_{2}Y_{t-2} + \dots + \phi_{q}Y_{t-q} = c+{e_t+\sum_{i=1}^p \phi_iY_{t-i}} $$ Terlihat bahwa $Y_t$ berperan penting dalam pembangkitan proses AR.

## Pembangkitan Proses AR

Akan dicoba membangkitkan proses AR paling sederhana, yaitu AR(1) dengan $\phi = 0.7$ sebanyak 200 observasi dan $c=0$.

```{r}
set.seed(1234)
```

Nilai-nilai selanjutnya dapat dicari melalui *loop*. Bentuk loop dapat dilihat dari rumus AR(1) yang hendak dibangkitkan:

$$ Y_t = e_t+0.7Y_{t-1} $$

```{r}
n<-length(wn)
n
ar <- c(1:n) 
for (i in 2:n) {ar[i]<-wn[i]+0.7*ar[i-1]}
ar
```

Selain menggunakan cara di atas, pembangkitan proses AR dapat dilakukan dengan fungsi `arima.sim()` sebagai berikut.

```{r}
ar1 <- arima.sim(list(order=c(1,0,0), ar=0.7), n=200)
ar1
```

## Karakteristik AR(1)

### Plot Time Series

```{r}
ts.plot(ar)
```

Berdasarkan plot time series tersebut terlihat bahwa data cenderung stasioner pada rataan

### Plot ACF

```{r}
acf(ar)
```

Berdasarkan plot ACF tersebut terlihat bahwa plot ACF cenderung *tails off* dan cenderung membentuk pola grafik sinus

### Plot PACF

```{r}
pacf(ar)
```

Berdasarkan plot PACF tersebut, terlihat bahwa plot PACF *cuts off* pada lag pertama, sejalan dengan teori yang ada

### Plot EACF

```{r}
TSA::eacf(ar)
```

Berdasarkan pola segitiga nol pada plot EACF, terlihat bahwa segitiga nol berada pada ordo AR(1) dan ordo MA(0)

### Scatterplot Antar Lag

#### Korelasi antara $Y_t$ dengan $Y_{t-1}$

```{r}
#Yt
yt_ar <- ar[-1]
yt_ar
#Yt-1
yt_1_ar <- ar[-200]
yt_1_ar
```

```{r}
plot(y=yt_ar,x=yt_1_ar)
```

Berdasarkan scatterplot tersebut, terlihat bahwa terdapat hubungan positif antara $Y_t$ dengan $Y_{t-1}$. Hal ini sesuai dengan teori yang ada

```{r}
cor(yt_ar,yt_1_ar)
```

Korelasi antara $Y_t$ dengan $Y_{t-1}$ dari hasil simulasi mendekati perhitungan teoritis yaitu $\rho_1=\phi^1=0.7$

#### Korelasi antara $Y_t$ dengan $Y_{t-2}$

```{r}
#Yt
yt_ar2 <- ar[-c(1,2)]
yt_ar2
#Yt-2
yt_2_ar <- ar[-c(199,200)]
yt_2_ar
```

```{r}
plot(y=yt_ar2,x=yt_2_ar)
```

Berdasarkan scatterplot tersebut, terlihat bahwa terdapat hubungan positif antara $Y_t$ dengan $Y_{t-2}$. Hal ini sesuai dengan teori yang ada

```{r}
cor(yt_ar2,yt_2_ar)
```

Korelasi antara $Y_t$ dengan $Y_{t-2}$ dari hasil simulasi mendekati perhitungan teoritis yaitu $\rho_2=\phi^2=0.49$.

## Fungsi pembangkitan ARMA

Setelah mengetahui cara membangkitkan data berpola AR, MA, dan ARMA sederhana, bagaimana cara melakukan pembangkitan data berpola tersebut yang lebih kompleks? Apakah dapat dibuat suatu fungsi yang fleksibel yang memungkinan pembangkitan dengan berapapun jumlah koefisien?

Pertama, lihat kembali bentuk umum data berpola ARMA.

$$
y_{t} = c + \sum_{i=1}^p \phi_{i}y_{t-i} + \sum_{j=1}^q e_{t-j}+ e_{t}
$$

Komponen $c$ dan $e_{t}$ cukup mudah untuk dibuat dan dicari. Bagaimana untuk komponen AR dan MA? Bayangkan ada koefisien dan data sebagai berikut:

$$
\begin{aligned}
\begin{bmatrix}
\phi_1 \  \phi_2 \ \phi_3
\end{bmatrix}&=
\begin{bmatrix}
0.3 \ 0.5 \ 0.2
\end{bmatrix}
\\
\begin{bmatrix}
y_{t-1} \  y_{t-2} \ y_{t-3}
\end{bmatrix}&=
\begin{bmatrix}
1 \ 2 \ 3
\end{bmatrix}
\end{aligned}
$$

Maka dari itu,

$$
\begin{aligned}
\begin{bmatrix}
\phi_1 \  \phi_2 \ \phi_3
\end{bmatrix}
\begin{bmatrix}
y_{t-1} \\  y_{t-2} \\ y_{t-3}
\end{bmatrix} &= \phi_1 \ y_{t-1}+\phi_2 \ y_{t-2}+\phi_3 \ y_{t-3}
\\
\begin{bmatrix}
 0.3 \ 0.5 \ 0.2
\end{bmatrix}
\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix} & = 0.3 \cdot1+0.5 \cdot 2+0.2 \cdot 3\\
&=0.3+1+0.6 = 1.9
\end{aligned}
$$

Jika koefisien dan *white noise*/nilai deret waktu sebelumnya dapat diekstrak dalam bentuk vektor, dapat dilakukan perkalian matriks untuk mencari nilai bagian AR dan MA:

```{r}
set.seed(30)
coefs <- c(0.3, 0.5, 0.2)
e <- c(1, 2, 3)

coefs %*% e
```

Atau, dapat dilakukan perkalian *elementwise* yang dijumlahkan:

```{r}
coefs * e
sum(coefs * e)
```

Dari prinsip ini, dapat dibuat fungsi umum untuk membangkitkan data ARMA. Input dari fungsi adalah jumlah data yang hendak dibangkitkan, koefisien MA, dan koefisien AR

```{r}
arma.sim <- function(n, macoef, arcoef){
  manum <- length(macoef)
  arnum <- length(arcoef)
  stopifnot(manum < n & arnum < n)
  
  wn <- rnorm(n, sd = 0.5)
  init <- max(manum, arnum)

  arma <- wn[1:init]
  for(i in {init+1}:n){
   mastart <- i - manum
   maend <- i-1
   arstart <- i - arnum
   arend <- i-1
   arma[i] <- sum(arcoef * arma[arstart:arend]) + sum(macoef * wn[mastart:maend])  + wn[i]
   }
  return(arma)
}
```

Terlihat bahwa komponen $\sum_{i=1}^q y_{t-1}$ disimulasikan melalui `sum(arcoef * arma[arstart:arend])`. Jadi, koefisien dikalikan dengan data $y$ dari $t-q$ di mana q adalah jumlah koefisien AR, sampai data $t-1$. Lalu komponen $\sum_{j=1}^q e_{t-j}$ disimulasikan melalui `sum(macoef * wn[mastart:maend])`. Koefisien dikalikan dengan *white noise* $e$ dari $t-p$, p jumlah koefisien MA, sampai $t-1$.

```{r}
# beberapa contoh pembangkitan melalui fungsi

ma3 <- arma.sim(1000, c(0.4, 0.6,0.4), 0)
ar2 <- arma.sim(1000, 0, c(0.3, 0.6))

par(mfrow = c(2, 2))
acf(ma3)
pacf(ma3)
acf(ar2)
pacf(ar2)
```

```{r}
#contoh untuk ARMA
arma22 <- arma.sim(2000, c(0.3, 0.4), c(0.3,0.4))

arma22 |> arima(c(2,0,2))
```

```{r}
set.seed(1234)
n = length(wn)
phi1 = 0.7
theta1 = 0.5

y.arma=c(1:n)
for (i in 2:n){y.arma[i] = phi1*y.arma[i-1] + theta1*wn[i-1]+wn[i]}
y.arma
```

Pembangkitan ARMA(p,q) juga dapat dilakukan dengan fungsi `arima.sim` sebagai berikut.

```{r}
arma11 <- arima.sim(list(order=c(1,0,1), ar = 0.7, ma = 0.5), n=200)
arma11
```

## Karakteristik ARMA(1,1)

### Plot Time Series

```{r}
par(mfrow = c(1, 2))
ts.plot(y.arma)
ts.plot(arma11)
par(mfrow = c(1, 1))
```

Berdasarkan plot time series tersebut, terlihat bahwa model ARMA(1,1) cenderung stasioner dalam rataan

### Plot ACF

```{r}
par(mfrow = c(1, 2))
acf(y.arma)
acf(arma11)
par(mfrow = c(1, 1))
```

Berdasarkan plot ACF tersebut, terlihat bahwa model ARMA(1,1) hasil simulasi memiliki plot ACF yang *tails off*, sesuai dengan teori yang ada

### Plot PACF

```{r}
par(mfrow = c(1, 2))
pacf(y.arma)
pacf(arma11)
par(mfrow = c(1, 1))
```

Berdasarkan plot PACF tersebut, terlihat bahwa model ARMA(1,1) hasil simulasi memiliki plot PACF yang *tails off*, sesuai dengan teori

### Plot EACF

```{r}
TSA::eacf(y.arma)
TSA::eacf(arma11)
```

Berdasarkan pola segitiga nol pada plot EACF, terlihat bahwa segitiga nol berada pada ordo AR(1) dan ordo MA(1)

### Scatterplot Antar Lag

#### Korelasi antara $Y_t$ dengan $Y_{t-1}$

```{r}
#Yt
yt_arma <- arma11[-1]
yt_arma
#Yt-1
yt_1_arma <- arma11[-200]
yt_1_arma
```

```{r}
plot(y=yt_arma,x=yt_1_arma)
```

Berdasarkan scatterplot tersebut, terlihat bahwa terdapat hubungan positif antara $Y_t$ dengan $Y_{t-1}$. Hal ini sesuai dengan teori yang ada

```{r}
cor(yt_arma,yt_1_arma)
```

Korelasi antara $Y_t$ dengan $Y_{t-1}$ dari hasil simulasi mendekati perhitungan teoritis. $$
\rho_k = \frac{(1-\theta \phi)(\phi-\theta)}{1-2\theta\phi+\theta^2}\phi^{k-1} \:untuk \:k\ge1
$$

$$
\rho_1 = \frac{(1-((-0.5)(0.7)))(0.7-(-0.5))}{1-2(-0.5)(0.7)+(-0.5)^2}0.7^{1-1}=0,8307692 $$

# Pertemuan 5

```{r}
library(ggplot2)
library(tsibble)
library(tseries)
library(MASS)
```

```{r}
set.seed(8990)
```

## Stasioner dalam Rataan dan Ragam

Pada dasarnya, pembangkitan data ARIMA akan menghasilkan data yang stasioner dalam rataan dan ragam karena akan mengikuti fungsi *default*-nya yang mengikuti pembangkitan bilangan acak normal dengan `mean=0` dan `ragam=1` .

```{r}
stas <- arima.sim(n=200, list(order=c(1,0,1),ar= .2, ma=.2),mean=12)
```

### Plot *Time Series*

```{r}
plot_stas <- stas |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
plot_stas
mean(stas)
```

Plot deret waktu di atas menunjukkan bahwa data stasioner dalam rataan, ditandai dengan data yang menyebar di sekitar nilai tengahnya (18) dan stasioner dalam ragam, ditandai dengan lebar pita yang cenderung sama.

### Plot ACF

```{r}
acf(stas)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut cenderung *tails off* dan membentuk gelombang sinus.

### Uji ADF

```{r}
tseries::adf.test(stas)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01 yang lebih kecil dari taraf nyata 5% sehingga tolak $H_0$ dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF.

### Plot Box-Cox

```{r}
index <- seq(1:200)
bc = boxcox(stas~index, lambda = seq(0,4,by=0.01))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

Gambar di atas menunjukkan nilai *rounded value* ($\lambda$) optimum sebesar **1,97** dan pada selang kepercayaan 95% nilai memiliki batas bawah **0,22** dan batas atas **3,73**. Selang tersebut memuat nilai satu sehingga dapat dikatakan bahwa data bangkitan stasioner dalam ragam.

### Partisi Data

#### Bagian 1

```{r}
dt_stas1 <- stas[1:67] |> ts()
mean(dt_stas1)
var(dt_stas1)
```

#### Plot Time Series

```{r}
dt_stas1 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Plot deret waktu di atas menunjukkan bahwa data stasioner dalam rataan, ditandai dengan data yang menyebar di sekitar nilai tengahnya (18) dan stasioner dalam ragam, ditandai dengan lebar pita yang cenderung sama.

#### Plot ACF

```{r}
acf(dt_stas1)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut cenderung *tails off* dan membentuk gelombang sinus.

#### Uji ADF

```{r}
tseries::adf.test(dt_stas1)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.043 yang lebih kecil dari taraf nyata 5% sehingga tolak $H_0$ dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF.

#### Plot Boxcox

```{r}
index <- seq(1:67)
bc = boxcox(dt_stas1~index, lambda = seq(-2,6,by=1))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

Gambar di atas menunjukkan nilai *rounded value* ($\lambda$) optimum sebesar **2,2** dan pada selang kepercayaan 95% nilai memiliki batas bawah **-1,03** dan batas atas **5,52**. Selang tersebut memuat nilai satu sehingga dapat dikatakan bahwa data bangkitan stasioner dalam ragam.

#### Bagian 2

```{r}
dt_stas2 <- stas[1:134] |> ts()
mean(dt_stas2)
var(dt_stas2)
```

#### Plot Time Series

```{r}
dt_stas2 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Plot deret waktu di atas menunjukkan bahwa data stasioner dalam rataan, ditandai dengan data yang menyebar di sekitar nilai tengahnya (18) dan stasioner dalam ragam, ditandai dengan lebar pita yang cenderung sama.

#### Plot ACF

```{r}
acf(dt_stas2)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut cenderung *tails off* dan membentuk gelombang sinus.

#### Uji ADF

```{r}
adf.test(dt_stas2)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01 yang lebih kecil dari taraf nyata 5% sehingga tolak $H_0$ dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF.

#### Plot Boxcox

```{r}
index <- seq(1:134)
bc = boxcox(dt_stas2~index, lambda = seq(0,6,by=1))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

Gambar di atas menunjukkan nilai *rounded value* ($\lambda$) optimum sebesar **2,85** dan pada selang kepercayaan 95% nilai memiliki batas bawah **0,48** dan batas atas **5,27**. Selang tersebut memuat nilai satu sehingga dapat dikatakan bahwa data bangkitan stasioner dalam ragam.

## Tidak Stasioner dalam Rataan, Stasioner dalam Ragam

Bagaimana cara mensimulasikan data dengan tren tertentu?

Kunci dari simulasi tersebut berada di $Y_{t}-Y_{t-1}$, atau *first difference*, atau selisih antara observasi di waktu ke $t$ dan observasi sebelumnya. Jika suatu deret waktu memiliki tren naik, misal, maka selisih tersebut akan positif. Sebaliknya, jika suatu deret memiliki tren turun, maka selisih akan negatif. Jika suatu deret stasioner, selisih akan memiliki rata-rata nol.

Ini dapat diilustrasikan dengan fungsi `cumsum`. `cumsum` adalah jumlah kumulatif. Untuk mengerti logika dari pengunaan jumlah kumulatif, bayangkan ada deret waktu dengan nilai awal $c$:

$$
Y_1 = c
$$

Lalu definsikan $d_i$, di mana $i = 2, 3, \ldots$ sebagai selisih observasi ke-i dengan observasi sebelumnya:

$$
d_i = Y_i-Y_{i-1}
$$

Perhatikan bahwa:

$$
\begin{aligned}
Y_3-Y_{1} & = d_3+d_2\\
&= Y_3-Y_2+Y_2-Y_1
\end{aligned}
$$

Cukup jelas bahwa sifat tersebut berarti:

$$
\begin{aligned}
Y_t-Y_1 &= \sum_{i=2}^t d_i\\
Y_t &= Y_1 + \sum_{i=2}^t d_i
\end{aligned}
$$

Atau, amatan di waktu ke $t$ dapat ditemukan dari menambahkan amatan ke-1 dengan jumlah kumulatif perbedaan $d_i$ sampai di waktu ke-t. Kode di bawah membuat data dengan proses tersebut. Ada tiga skenario, yaitu:

1.  Selisih antara observasi dan observasi sebelumnya nol

2.  Selisih antara observasi dan observasi sebelumnya positif

3.  Selisih antara observasi dan observasi sebelumnya negatif

Dengan nilai awal 1 dan komponen $e_t$ menyebar normal.

```{r}
notrend <- 1 + cumsum(rep(0, 100)) + rnorm(100) |> ts()
postrend <- 1 + cumsum(rep(0.2, 100)) + rnorm(100) |> ts() 
negtrend <- 1 + cumsum(rep(-0.2, 100)) + rnorm(100) |> ts()
```

Hasil yang muncul dapat di-plot (note, untuk plotting ini digunakan `ggplot2`; `ts.plot()` dapat digunakan - ini tergantung preferensi saja):

```{r}
plot_notrend <- notrend |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai") + ggtitle("First Difference = 0")
plot_postrend <- postrend |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai") + ggtitle("First Difference = 0.2")
plot_negtrend <- negtrend |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai") + ggtitle("First Difference = -0.2")

ggpubr::ggarrange(plot_notrend, plot_postrend, plot_negtrend, nrow = 3)
```

Dapat disimulasikan proses MA atau AR yang tidak stasioner dengan suatu tren dengan mensimulasikan beda terlebih dahulu (menggunakan `arima.sim`), ditambah suatu konstanta, lalu mencari jumlah kumulatif. Terdapat juga parameter `mean` di fungsi `arima.sim`, tetapi parameter ini adalah parameter untuk $E[e_t]$, bukan $E[Y_t-Y_{t-1}]$. Proses dari nilai harapan *white noise* tertentu menjadi nilai harapan $Y$ di suatu proses MA atau AR yang tak stasioner sangat tergantung pada model, yang diilustrasikan [di sini](https://stats.stackexchange.com/questions/294748/arima-sim-again-what-does-the-mean-parameter-do-in-an-ma1-sim/).

```{r}
startSpot <- 3
dt <- {arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), sd=2) + 1.5}  |> ts()
yt <- startSpot + cumsum(dt) |> ts()

dt_alt <- arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), mean = 1.5, sd=2)  |> ts()
yt_alt <- startSpot + cumsum(dt_alt) |> ts()

plot_dt <- dt |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai") + ggtitle("Penambahan konstanta 1.5 di selisih")
plot_yt <- yt |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
plot_dt_alt <- dt_alt |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai") + ggtitle("Mean 1.5 di parameter arima.sim")
plot_yt_alt <- yt_alt |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) + geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")

ggpubr::ggarrange(plot_dt, plot_yt, plot_dt_alt, plot_yt_alt)
```

Terlihat di contoh di atas bahwa menambahkan konstanta 1.5 pada hasil `arima.sim` beda dengan memasukkan parameter `mean = 1.5`.

### Data Bangkitan Baru

```{r}
set.seed(8990)
dt_alt <- arima.sim(n=200, list(order=c(1,0,1),ar=c(.2), ma=.2), mean=0.2, sd=0.5)  |> ts()
postrend <- startSpot + cumsum(dt_alt) |> ts()
```

### Plot Time Series

```{r}
postrend |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
mean(postrend)
var(postrend)
```

Berdasarkan plot time series tersebut, terlihat bahwa data tidak stasioner dalam rataan, ditandai dengan adanya tren positif, tetapi stasioner dalam ragam, ditandai dengan adanya lebar pita pada plot yang cenderung sama.

```{r}
ts.plot(diff(postrend))
```

### Plot ACF

```{r}
acf(postrend)
```

### Uji ADF

```{r}
adf.test(postrend)
```

### Plot Box-Cox

```{r}
index <- seq(1:200)
bc = boxcox(postrend~index, lambda = seq(0.8,1.1,by=0.001))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

### Partisi Data

#### Bagian 1

```{r}
postrend1 <- postrend[1:100] |> ts()
mean(postrend1)
var(postrend1)
```

#### Plot Time Series

```{r}
postrend1 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line()+theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

#### Plot ACF

```{r}
acf(postrend1)
```

#### Uji ADF

```{r}
adf.test(postrend1)
```

#### Plot Boxcox

```{r}
index <- seq(1:100)
bc = boxcox(postrend1~index, lambda = seq(0.5,1,by=0.001))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

#### Bagian 2

```{r}
postrend2 <- postrend[101:200] |> ts()
mean(postrend2)
var(postrend2)
```

#### Plot Time Series

```{r}
postrend2 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

#### Plot ACF

```{r}
acf(postrend2)
```

#### Uji ADF

```{r}
adf.test(postrend2)
```

#### Plot Boxcox

```{r}
index <- seq(1:100)
bc = boxcox(postrend2~index, lambda = seq(-1,1.2,by=0.001))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

## Tidak Stasioner dalam Ragam, Stasioner dalam Rataan

`arima.sim` memiliki parameter sd yang dapat di-set beda.

```{r}
set.seed(9089)
sd1 <- arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), sd = 1)
sd5 <- arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), sd = 5)
dtgab <- c(sd1,sd5) |> ts()
```

### Plot Time Series

```{r}
dtgab |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data stasioner dalam rataan, ditandai dengan tidak adanya trend ataupun musiman pada data, namun tidak stasioner dalam ragam, ditandai dengan adanya perbedaan lebar pita pada plot

### Plot ACF

```{r}
acf(dtgab)
```

Berdasarkan plot ACF, terlihat bahwa data stasioner dalam rataan, ditandai dengan plot ACF yang *tails off* dan cenderung membentuk gelombang sinus

### Uji ADF

```{r}
adf.test(dtgab)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01 yang kurang dari taraf nyata 5% dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

### Partisi Data

#### Bagian 1

```{r}
dtgab1 <- dtgab[1:66] |> ts()
mean(dtgab1)
var(dtgab1)
```

#### Plot Time Series

```{r}
dtgab1 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data cenderung stasioner dalam rataan, ditandain dengan tidak adanya trend dan musiman pada data, serta stasioner dalam ragam, ditandai dengan lebar pita yang cenderung sama pada plot tersebut

#### Plot ACF

```{r}
acf(dtgab1)
```

Berdasarkan plot ACF, terlihat bahwa data cenderung stasioner dalam rataan ditandai dengan plot ACF yang *tails off* dan cenderung membentuk gelombang sinus

#### Uji ADF

```{r}
adf.test(dtgab1)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01691 yang kurang dari taraf nyata 5% dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

#### Plot Boxcox

#### Bagian 2

```{r}
dtgab2 <- dtgab[1:132] |> ts()
mean(dtgab2)
var(dtgab2)
```

#### Plot Time Series

```{r}
dtgab2 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot data deret waktu tersebut, terlihat bahwa data stasioner dalam rataan, ditandai dengan data yang tidak menunjukkan adanya trend ataupun musiman, serta tidak stasioner dalam ragam ditandai dengan lebar pita pada plot yang cenderung berbeda di beberapa periode waktunya

#### Plot ACF

```{r}
acf(dtgab2)
```

Berdasarkan plot ACF tersebut, terlihat bahwa data stasioner dalam rataan ditandai dengan plot ACF yang *cuts off* pada lag ke 2

#### Uji ADF

```{r}
adf.test(dtgab2)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01383 yang kurang dari taraf nyata 5% dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

## Tidak Stasioner dalam Rataan dan Ragam

Hal ini dapat disimulasikan dengan membangkitkan data yang tidak stasioner dalam ragam lalu membentuk trend menggunakan data tersebut

```{r}
set.seed(8990)
sd2 <- arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), sd = 2)
sd6 <- arima.sim(n=100, list(order=c(1,0,1),ar=c(.2), ma=.2), sd = 6)
datagab <- c(sd2,sd6)
dt_rg <- startSpot + cumsum(datagab) |> ts() 
```

### Plot Time Series

```{r}
dt_rg |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data tidak stasioner dalam rataan, ditandai dengan adanya trend pada data dan tidak stasioner dalam ragam, ditandai dengan adanya perbedaan lebar pita pada plot

### Plot ACF

```{r}
acf(dt_rg)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut menurun secara perlahan (*tails off slowly*) yang menandakan data tidak stasioner dalam rataan

### Uji ADF

```{r}
adf.test(dt_rg)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.6447 yang lebih besar dari taraf nyata 5% dan menandakan bahwa data tidak stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

### Partisi Data

#### Bagian 1

```{r}
dt_rg1 <- dt_rg[1:66] |> ts()
mean(dt_rg1)
var(dt_rg1)
```

#### Plot Time Series

```{r}
dt_rg1 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data tidak stasioner dalam rataan karena masih terdapat tren pada data, namun cenderung stasioner dalam ragam karena memiliki lebar pita yang cenderung sama

#### Plot ACF

```{r}
acf(dt_rg1)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut menurun secara perlahan (*tails off slowly*) yang menandakan data tidak stasioner dalam rataan

#### Uji ADF

```{r}
adf.test(dt_rg1)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.6816 yang lebih besar dari taraf nyata 5% dan menandakan bahwa data tidak stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

#### Bagian 2

```{r}
dt_rg2 <- dt_rg[67:132] |> ts()
mean(dt_rg2)
var(dt_rg2)
```

#### Plot Time Series

```{r}
dt_rg2 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data tidak stasioner dalam rataan karena masih terdapat tren pada data, dan tidak stasioner dalam ragam karena memiliki lebar pita yang cenderung tidak sama

#### Plot ACF

```{r}
acf(dt_rg2)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut menurun secara perlahan (*tails off slowly*) yang menandakan data tidak stasioner dalam rataan

#### Uji ADF

```{r}
adf.test(dt_rg2)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.9202 yang lebih besar dari taraf nyata 5% dan menandakan bahwa data tidak stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

#### Bagian 3

```{r}
dt_rg3 <- dt_rg[133:200] |> ts()
mean(dt_rg3)
var(dt_rg3)
```

#### Plot Time Series

```{r}
dt_rg3 |> as_tsibble() |> 
  ggplot(aes(x = index, y = value)) +
  geom_line() + theme_bw() +
  xlab("Obs") + ylab("Nilai")
```

Berdasarkan plot time series tersebut, terlihat bahwa data tidak stasioner dalam rataan karena masih terdapat tren pada data, namun cenderung stasioner dalam ragam karena memiliki lebar pita yang cenderung sama

#### Plot ACF

```{r}
acf(dt_rg3)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF pada data tersebut menurun secara perlahan (*tails off slowly*)yang menandakan data tidak stasioner dalam rataan

#### Uji ADF

```{r}
tseries::adf.test(dt_rg3,k=5)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.323 yang lebih besar dari taraf nyata 5% dan menandakan bahwa data tidak stasioner dalam rataan. Hal ini tidak sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF

# Pertemuan 67

## Packages

```{r}
library(ggplot2)
library(tsibble)
library(tseries)
library(MASS)
library(forecast)
library(TSA)
library(TTR)
library(aTSA)
library(graphics)
```

## Data Bangkitan

### Pembangkitan Data

Data yang akan dibangkitkan adalah data dengan model MA(2) sebagai berikut.

```{r}
set.seed(99)
ma2 <- arima.sim(list(order = c(0,0,2), ma = c(0.55,0.65)), n = 175)
```

Data kemudian dibagi menjadi data latih dan data uji. Pembagian kali ini dilakukan dengan proporsi / perbandingan, yaitu 80:20.

```{r}
ma2 <- ma2[-c(1:25)]
ma2.train <- ma2[1:120]
ma2.test <- ma2[121:150]
```

### Eksplorasi Data

Sebelum masuk dalam tahap pemodelan, dilakukan eksplorasi data dengan plot deret waktu untuk melihat pola data.

```{r}
#--PLOT TIME SERIES--#
plot(ma2.train,
     col = "navyblue",
     lwd = 1,
     type = "o",
     xlab = "Time",
     ylab = "Data")
```

Berdasarkan plot data deret waktu di atas, terlihat data cenderung stasioner dalam rataan dan ragam. Data stasioner dalam rataan karena menyebar/bergerak di sekitar nilai tengahnya (0) dan dikatakan stasioner dalam ragam karena memiliki lebar pita yang cenderung sama. Selain dengan plot data deret waktu, akan dilakukan pengecekan stasioneritas data dengan plot ACF dan uji ADF.

```{r}
#--CEK KESTASIONERAN---#
acf(ma2.train, main="ACF", lag.max=20)
```

Berdasarkan plot ACF di atas, dapat dilihat bahwa plot *cuts off* pada *lag* ke-2. Hal ini sesuai dengan proses pembangkitan model MA(2).

```{r}
adf.test(ma2.train) 
#stasioner
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01358 yang lebih kecil dari taraf nyata 5% sehingga tolak $H_0$ dan menandakan bahwa data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF.

### Spesifikasi Model

```{r}
#---SPESIFIKASI MODEL---#
par(mfrow = c(1,2))
acf(ma2.train, main="ACF", lag.max=20) #ARIMA(0,0,2)
pacf(ma2.train, main="PACF", lag.max=20) #ARIMA(1,0,0)
par(mfrow = c(1,1))
```

Berdasarkan Plot ACF, terlihat *cuts off* pada lag ke-2 sehingga dapat kita asumsikan model yang terbentuk adalah ARIMA(0,0,2). Selanjutnya, berdasarkan plot PACF, terlihat *cuts off* pada lag pertama sehingga model yang terbentuk adalah ARIMA(1,0,0). Selain dengan plot ACF dan PACF, penentuan spesifikasi model dilakukan dengan *extended ACF* (EACF) berikut ini.

```{r}
eacf(ma2.train) 
#ARIMA(0,0,2) #ARIMA(1,0,3) #ARIMA(2,0,3) #ARIMA(3,0,3) 
#Terdapat 5 model tentatif
```

Menggunakan plot EACF, dapat diambil beberapa model dengan melihat ujung segitiga yang terbentuk, antara lain ARIMA(0,0,2), ARIMA(1,0,3), ARIMA(2,0,3), dan ARIMA(3,0,3).

### Pendugaan Parameter

Selanjutnya akan dilakukan pendugaan parameter kelima model ARIMA yang terbentuk sebelumnya. Pendugaan dilakukan dengan fungsi `Arima()` yang dilanjutkan dengan melihat nilai AIC pada ringkasan data dan melihat signifikansi parameter.

```{r}
#---PENDUGAAN PARAMETER MODEL---#
model1.ma2=Arima(ma2.train, order=c(0,0,2),method="ML")
summary(model1.ma2) #AIC=326.87
lmtest::coeftest(model1.ma2) #seluruh parameter signifikan

model2.ma2=Arima(ma2.train, order=c(1,0,0),method="ML") 
summary(model2.ma2) #AIC=340.47
lmtest::coeftest(model2.ma2) #seluruh parameter signifikan

model3.ma2=Arima(ma2.train, order=c(1,0,3),method="ML") 
summary(model3.ma2) #AIC=329.22
lmtest::coeftest(model3.ma2) #tidak ada yang signifikan

model4.ma2=Arima(ma2.train, order=c(2,0,3),method="ML") 
summary(model4.ma2) #AIC=330.6
lmtest::coeftest(model4.ma2) #hanya ma2 yang signifikan

model5.ma2=Arima(ma2.train, order=c(3,0,3),method="ML") 
summary(model5.ma2) #AIC=329.87
lmtest::coeftest(model5.ma2) #hanya ma1 dan ma2 yang signifikan

#model yang dipilih adalah model 1, yaitu ARIMA(0,0,2)
```

Berdasarkan pendugaan parameter di atas, nilai AIC terkecil dimiliki oleh model ARIMA(0,0,2) dan parameter model ARIMA(0,0,2) juga seluruhnya signifikan sehingga model yang dipilih adalah model ARIMA(0,0,2).

### Analisis Sisaan

Model terbaik hasil identifikasi kemudian dicek asumsi sisaannya. Sisaan model ARIMA harus memenuhi asumsi normalitas, kebebasan, dan kehomogenan ragam. Diagnostik model dilakukan secara eksplorasi dan uji formal.

#### Eksplorasi Sisaan

```{r}
#Eksplorasi
sisaan.ma2 <- model1.ma2$residuals
par(mfrow=c(2,2))
qqnorm(sisaan.ma2)
qqline(sisaan.ma2, col = "blue", lwd = 2)
plot(c(1:length(sisaan.ma2)),sisaan.ma2)
acf(sisaan.ma2)
pacf(sisaan.ma2)
par(mfrow = c(1,1))
```

Berdasarkan plot kuantil-kuantil normal, secara eksplorasi ditunjukkan sisaan menyebar normal mengikuti garis $45^{\circ}$. Kemudian dapat dilihat juga lebar pita sisaan yang cenderung sama menandakan bahwa sisaan memiliki ragam yang homogen. Akan tetapi, plot ACF dan PACF sisaan ARIMA(0,0,2) signifikan pada lag ke-6 sehingga sisaan tidak saling bebas. Kondisi ini akan diuji lebih lanjut dengan uji formal.

#### Uji Formal

```{r}
#1) Sisaan Menyebar Normal
ks.test(sisaan.ma2,"pnorm") 
#tak tolak H0 > sisaan menyebar normal
```

Selain dengan eksplorasi, asumsi tersebut dapat diuji menggunakan uji formal. Pada tahapan ini uji formal yang digunakan untuk normalitas adalah uji Kolmogorov-Smirnov (KS). Hipotesis pada uji KS adalah sebagai berikut.

$H_0$ : Sisaan menyebar normal

$H_1$ : Sisaan tidak menyebar normal

Berdasarkan uji KS tersebut, didapat *p-value* sebesar 0.9788 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa sisaan menyebar normal. Hal ini sesuai dengan hasil eksplorasi menggunakan plot kuantil-kuantil normal.

```{r}
#2) Sisaan saling bebas/tidak ada autokorelasi
Box.test(sisaan.ma2, type = "Ljung") 
#tak tolak H0 > sisaan saling bebas
```

Selanjutnya akan dilakukan uji formal untuk kebebasan sisaan menggunakan uji Ljung-Box. Hipotesis yang digunakan adalah sebagai berikut.

$H_0$ : Sisaan saling bebas

$H_1$ : Sisaan tidak tidak saling bebas

Berdasarkan uji Ljung-Box tersebut, didapat *p-value* sebesar 0.5082 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa sisaan saling bebas. Hal ini berbeda dengan eksplorasi.

```{r}
#3) Sisaan homogen
Box.test((sisaan.ma2)^2, type = "Ljung") 
#tak tolak H0 > sisaan homogen
```

Hipotesis yang digunakan untuk uji kehomogenan ragam adalah sebagai berikut.

$H_0$ : Ragam sisaan homogen

$H_1$ : Ragam sisaan tidak homogen

Berdasarkan uji Ljung-Box terhadap sisaan kuadrat tersebut, didapat *p-value* sebesar 0.116 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa ragam sisaan homogen.

```{r}
#4) Nilai tengah sisaan sama dengan nol
t.test(sisaan.ma2, mu = 0, conf.level = 0.95) 
#tak tolak h0 > nilai tengah sisaan sama dengan 0
```

Terakhir, dengan uji-t, akan dicek apakah nilai tengah sisaan sama dengan nol. Hipotesis yang diujikan sebagai berikut.

$H_0$ : nilai tengah sisaan sama dengan 0

$H_1$ : nilai tengah sisaan tidak sama dengan 0

Berdasarkan uji-ttersebut, didapat *p-value* sebesar 0.9594 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa nilai tengah sisaan sama dengan nol. Hal ini berbeda dengan eksplorasi.

### Overfitting

Tahapan selanjutnya adalah *overfitting* dilakukan dengan menaikkan orde AR(p) dan MA(q) dari model ARIMA(0,0,2) untuk melihat apakah terdapat model lain yang lebih baik dari model saat ini. Kandidat model *overfitting* adalah ARIMA(1,0,2) dan ARIMA(0,0,3).

```{r}
#---OVERFITTING---#
model1a.ma2=Arima(ma2.train, order=c(1,0,2),method="ML")
summary(model1a.ma2) #327.31
lmtest::coeftest(model1a.ma2) #ar1 tidak signifikan

model1b.ma2=Arima(ma2.train, order=c(0,0,3),method="ML")
summary(model1b.ma2) #327.24
lmtest::coeftest(model1b.ma2) #ma3 tidak signifikan

#model yang dipilih adalah model awal, yaitu ARIMA(0,0,2)
```

Berdasarkan kedua model hasil *overfitting* di atas, model ARIMA(1,0,2) dan ARIMA(0,0,3) memiliki AIC yang lebih besar dibandingkan dengan model ARIMA(0,0,2) dan parameter kedua model ARIMA(1,0,2) dan ARIMA(0,0,3) tidak seluruhnya signifikan. Oleh karena itu, model ARIMA(0,0,2) akan tetap digunakan untuk melakukan peramalan.

### Peramalan

Peramalan dilakukan menggunakan fungsi `forecast()` . Contoh peramalan berikut ini dilakukan untuk 30 hari ke depan.

```{r}
#---FORECAST---#
ramalan <- forecast(model1.ma2, h = 30) 
ramalan
data.ramalan <- ramalan$mean
plot(ramalan)
```

Berdasarkan hasil plot ramalan di atas, dapat dilihat bahwa ramalan ARIMA(0,0,2) cenderung meningkat di awal periode dan stabil hingga akhir periode. Selanjutnya, dapat dicari nilai akurasi antara hasil ramalan dengan data uji sebagai berikut.

```{r}
perbandingan<-matrix(data=c(ma2.test, data.ramalan),
                     nrow = 30, ncol = 2)
colnames(perbandingan)<-c("Aktual","Hasil Forecast")
perbandingan
accuracy(data.ramalan, ma2.test)
```

## Data Asli

Digunakan data kurs yang dalam hal ini hanya digunakan data 500 periode awal

```{r}
datakurs<-read.csv("C:/Users/acer/OneDrive - apps.ipb.ac.id/Documents/S2_Semester 3/MPDW/MPDW/Praktikum Pertemuan 8-2022/kurs.csv",header=T)
datakurs <- datakurs[1:500,]
datakurs.ts<-ts(datakurs)
```

### Eksplorasi Data

#### Plot Data Penuh

```{r}
plot.ts(datakurs.ts, lty=1, xlab="waktu", ylab="Kurs", main="Plot Data Kurs")
```

Berdasarkan plot data deret waktu, terlihat bahwa data cenderung memiliki trend yang naik. Berdasarkan pola data, pembagian data latih dan data uji ditetapkan dengan proporsi 86%:14%.

#### Plot Data Latih

```{r}
kurstrain<-datakurs[1:430]
train.ts<-ts(kurstrain)
plot.ts(train.ts, lty=1, xlab="waktu", ylab="Kurs", main="Plot Kurs Train")
```

Berdasarkan plot data deret waktu pada data latih, terlihat bahwa data cenderung memiliki trend yang naik dan cenderung tidak bergerak pada nilai tengah tertentu. Hal ini mengindikasikan bahwa data tidak stasioner dalam rataan.

#### Plot Data Uji

```{r}
kurstest<-datakurs[431:500]
test.ts<-ts(kurstest)
plot.ts(test.ts, lty=1, xlab="waktu", ylab="Kurs", main="Plot Kurs Test")
```

### Uji Stasioneritas Data

#### Plot ACF

```{r}
acf(train.ts)
```

Berdasarkan plot ACF, terlihat bahwa plot ACF data menurun secara perlahan (*tails of slowly*). Hal ini juga menjadi indikasi bahwa data tidak stasioner dalam rataan

#### Uji ADF

```{r}
tseries::adf.test(train.ts)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.5553 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa data tidak stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF, sehingga ketidakstasioneran model kedepannya harus ditangani

#### Plot Box-Cox

```{r}
index <- seq(1:430)
bc = boxcox(train.ts~index, lambda = seq(5,10,by=1))
#Nilai Rounded Lambda
lambda <- bc$x[which.max(bc$y)]
lambda
#SK
bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]
```

Plot Boxcox menunjukkan nilai *rounded value* ($\lambda$) optimum sebesar **6,64** dan pada selang kepercayaan 95% nilai memiliki batas bawah **0,48** dan batas atas **5,27**. Selang tersebut memuat nilai satu sehingga dapat dikatakan bahwa data bangkitan stasioner dalam ragam.

### Penanganan Ketidakstasioneran Data

```{r}
train.diff<-diff(train.ts,differences = 1) 
plot.ts(train.diff, lty=1, xlab="waktu", ylab="Data Difference 1 Kurs", main="Plot Difference Kurs")
```

Berdasarkan plot data deret waktu, terlihat bahwa data sudah stasioner dalam rataan ditandai dengan data bergerak pada nilai tengah tertentu (tidak terdapat trend ataupun musiman pada data)

#### Plot ACF

```{r}
acf(train.diff)
```

Berdasarkan plot tersebut, terlihat bahwa plot ACF cuts off pada lag ke 1. Hal ini menandakan data sudah stasioner dalam rataan dan ketidakstasioneran data telah berhasil tertangani.

#### Uji ADF

```{r}
tseries::adf.test(train.diff)
```

$H_0$ : Data tidak stasioner dalam rataan

$H_1$ : Data stasioner dalam rataan

Berdasarkan uji ADF tersebut, didapat *p-value* sebesar 0.01 yang lebih kecil dari taraf nyata 5% sehingga tolak $H_0$ atau data stasioner dalam rataan. Hal ini sesuai dengan hasil eksplorasi menggunakan plot time series dan plot ACF, sehingga dalam hal ini ketidakstasioneran data sudah berhasil ditangani dan dapat dilanjutkan ke pemodelan

### Identifikasi Model

#### Plot ACF

```{r}
acf(train.diff)
```

Berdasarkan plot tersebut, terlihat bahwa plot ACF cenderung *cuts off* pada lag ke 1, sehingga jika plot PACF dianggap *tails of*, maka model tentatifnya adalah ARIMA(0,1,1).

#### Plot PACF

```{r}
pacf(train.diff)
```

Berdasarkan plot tersebut, terlihat bahwa plot PACF cenderung *cuts off* pada lag ke 1, sehingga jika plot ACF dianggap *tails of*, maka model tentatifnya adalah ARIMA(1,1,0).

Jika baik plot ACF maupun plot PACF keduanya dianggap tails of, maka model yang terbentuk adalah ARIMA(1,1,1)

#### Plot EACF

```{r}
eacf(train.diff)
```

Identifikasi model menggunakan plot EACF dilakukan dengan melihat ujung segitiga pada pola segitiga nol. Dalam hal ini model tentatif yang terbentuk adalah ARIMA(0,1,2), ARIMA(1,1,2), ARIMA(2,1,2), dan ARIMA(3,1,2).

### Pendugaan Parameter Model Tentatif

#### ARIMA(0,1,1)

```{r}
model1.da=Arima(train.diff, order=c(0,1,1),method="ML")
summary(model1.da) #AIC=4753.18
lmtest::coeftest(model1.da) #seluruh parameter signifikan
```

#### ARIMA(1,1,0)

```{r}
model2.da=Arima(train.diff, order=c(1,1,0),method="ML")
summary(model2.da) #AIC=4917.41
lmtest::coeftest(model2.da) #seluruh parameter signifikan
```

#### ARIMA(1,1,1)

```{r}
model3.da=Arima(train.diff, order=c(1,1,1),method="ML")
summary(model3.da) #AIC=4761.39
lmtest::coeftest(model3.da) #seluruh parameter signifikan
```

#### ARIMA(0,1,2)

```{r}
model4.da=Arima(train.diff, order=c(0,1,2),method="ML")
summary(model4.da) #AIC=4748.3
lmtest::coeftest(model4.da) #seluruh parameter signifikan
```

#### ARIMA(1,1,2)

```{r}
model5.da=Arima(train.diff, order=c(1,1,2),method="ML")
summary(model5.da) #AIC=4749.85 
lmtest::coeftest(model5.da) #terdapat parameter tidak signifikan
```

#### ARIMA(2,1,2)

```{r}
model6.da=Arima(train.diff, order=c(2,1,2),method="ML")
summary(model6.da) #AIC=4749.52
lmtest::coeftest(model6.da) #terdapat parameter tidak signifikan
```

Berdasarkan pendugaan parameter di atas, nilai AIC terkecil dimiliki oleh model ARIMA(0,1,2) dan parameter model ARIMA(0,1,2) juga seluruhnya signifikan sehingga model yang dipilih adalah model ARIMA(0,1,2).

### Analisis Sisaan

Model terbaik hasil identifikasi kemudian dicek asumsi sisaannya. Sisaan model ARIMA harus memenuhi asumsi normalitas, kebebasan sisaan, dan kehomogenan ragam. Diagnostik model dilakukan secara eksplorasi dan uji formal.

#### Eksplorasi Sisaan

```{r}
#Eksplorasi 
sisaan.da <- model4.da$residuals 
par(mfrow=c(2,2)) 
qqnorm(sisaan.da) 
qqline(sisaan.da, col = "blue", lwd = 2) 
plot(c(1:length(sisaan.da)),sisaan.da) 
acf(sisaan.da) 
pacf(sisaan.da) 
par(mfrow = c(1,1))
```

Berdasarkan plot kuantil-kuantil normal, secara eksplorasi ditunjukkan sisaan tidak menyebar normal ditandai dengan titik titik yang cenderung tidak mengikuti garis $45^{\circ}$. Kemudian dapat dilihat juga lebar pita sisaan yang cenderung tidak sama menandakan bahwa sisaan memiliki ragam yang heterogen. Plot ACF dan PACF sisaan ARIMA(0,0,2) juga tidak signifikan pada 20 lag awal yang menandakan saling bebas. Kondisi ini akan diuji lebih lanjut dengan uji formal.

#### Uji Formal

```{r}
#1) Sisaan Menyebar Normal 
ks.test(sisaan.da,"pnorm")  #tak tolak H0 > sisaan menyebar normal
```

Selain dengan eksplorasi, asumsi tersebut dapat diuji menggunakan uji formal. Pada tahapan ini uji formal yang digunakan untuk normalitas adalah uji Kolmogorov-Smirnov (KS). Hipotesis pada uji KS adalah sebagai berikut.

$H_0$ : Sisaan menyebar normal

$H_1$ : Sisaan tidak menyebar normal

Berdasarkan uji KS tersebut, didapat *p-value* sebesar 0.00 yang kurang dari taraf nyata 5% sehingga tolak $H_0$ dan menandakan bahwa sisaan tidak menyebar normal. Hal ini sesuai dengan hasil eksplorasi menggunakan plot kuantil-kuantil normal.

```{r}
#2) Sisaan saling bebas/tidak ada autokorelasi 
Box.test(sisaan.da, type = "Ljung")  #tak tolak H0 > sisaan saling bebas
```

Selanjutnya akan dilakukan uji formal untuk kebebasan sisaan menggunakan uji Ljung-Box. Hipotesis yang digunakan adalah sebagai berikut.

$H_0$ : Sisaan saling bebas

$H_1$ : Sisaan tidak tidak saling bebas

Berdasarkan uji Ljung-Box tersebut, didapat *p-value* sebesar 0.8471 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa sisaan saling bebas. Hal ini berbeda dengan eksplorasi.

```{r}
#3) Sisaan homogen 
Box.test((sisaan.da)^2, type = "Ljung")  #tak tolak H0 > sisaan homogen
```

Hipotesis yang digunakan untuk uji kehomogenan ragam adalah sebagai berikut.

$H_0$ : Ragam sisaan homogen

$H_1$ : Ragam sisaan tidak homogen

Berdasarkan uji Ljung-Box terhadap sisaan kuadrat tersebut, didapat *p-value* sebesar 0.000 yang kurang dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa ragam sisaan tidak homogen.

```{r}
#4) Nilai tengah sisaan sama dengan nol 
t.test(sisaan.da, mu = 0, conf.level = 0.95)  #tak tolak h0 > nilai tengah sisaan sama dengan 0
```

Terakhir, dengan uji-t, akan dicek apakah nilai tengah sisaan sama dengan nol. Hipotesis yang diujikan sebagai berikut.

$H_0$ : nilai tengah sisaan sama dengan 0

$H_1$ : nilai tengah sisaan tidak sama dengan 0

Berdasarkan uji-ttersebut, didapat *p-value* sebesar 0.4866 yang lebih besar dari taraf nyata 5% sehingga tak tolak $H_0$ dan menandakan bahwa nilai tengah sisaan sama dengan nol. Hal ini berbeda dengan eksplorasi.

### Peramalan

Peramalan dilakukan menggunakan fungsi `forecast()` . Contoh peramalan berikut ini dilakukan untuk 30 hari ke depan.

```{r}
#---FORECAST---#
ramalan.da <- forecast::forecast(model4.da, h = 30) 
ramalan.da
data.ramalan.da <- ramalan.da$mean
plot(ramalan.da)
```

Berdasarkan hasil plot ramalan di atas, dapat dilihat bahwa ramalan ARIMA(0,012) cenderung stabil hingga akhir periode. Selanjutnya, dapat dicari nilai akurasi antara hasil ramalan dengan data uji sebagai berikut.

```{r}
pt_1 <- train.ts[430] #nilai akhir data latih
hasil.forc.Diff <- data.ramalan.da
hasil <- diffinv(hasil.forc.Diff, differences = 1) + pt_1
#has.1 sama hasilnta dengan: cumsum(c(pt_1,hasil.forc.Diff))
ts.plot(train.ts,hasil)
```

```{r}
perbandingan.da<-matrix(data=c(head(test.ts, n=30), hasil[-1]),
                     nrow = 30, ncol = 2)
colnames(perbandingan.da)<-c("Aktual","Hasil Forecast")
perbandingan.da
accuracy(ts(hasil[-1]), head(test.ts, n=30))
```